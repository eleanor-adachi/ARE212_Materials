{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a665b9b",
   "metadata": {},
   "source": [
    "## Exercise 1.3 \n",
    "\n",
    "Sometimes we will encounter estimators (e.g., Maximum likelihood) that adopt an assumption of independence, rather than mean independence. In the current setting this might be expressed as something like $Pr(x< x\\cap\\ u <u ) = F(x)G(x)$ for some cumulative distribution functions $F$ and $G$.\n",
    "\n",
    "Show that independence implies mean independence, but not the converse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd34e7",
   "metadata": {},
   "source": [
    "#### 1. Independence Implies Mean Independence\n",
    "\n",
    "Supposing two random variables $X$ and $U$ wich they can be expresed as:\n",
    "$Pr(X < x \\cap\\ U <u) =Pr(X<x)⋅Pr(U<u)$\n",
    "\n",
    "Suposing the mean value of a function $h(X,U)$ under this independance assumption $$E[h(X,U)] = \\int\\int h(x,u) f_x(x) f_u(u) dx du=(\\int h(x,u)f_x(x)dx)⋅(\\int h(x,u)f_u(u)du)$$\n",
    "The separation of integrals is possible due to independence. Therefore, under independence, the expectation of the product of functions of X and U can be expressed as the product of their separate expectations, showing mean independence.\n",
    "\n",
    "#### 2. Mean Independence Does Not Imply Independence\n",
    "Let's consider two random variables, X and U, with covariance $Cov(X,U)=0$ but not necessarily independent. In this case, we can have $E[XU]=E[X]⋅E[U]=0$ This means that X and U are mean independent, but they are not necessarily independent. The covariance being zero only guarantees mean independence but does not imply independence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5387c",
   "metadata": {},
   "source": [
    "## Exercise 1.4\n",
    "Related to the previous: Show that while $u$ mean independent of $x$ implies $E(uh(x)=E(u)=0$, independence also implies $E(g(u)x)=Ex Eg(u)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d4274",
   "metadata": {},
   "source": [
    "Mean independence implies $E(uh(x))=E(u)=0$. \n",
    "Given mean independence between u and x, we have $E(uh(x))=E(u)⋅E(h(x))$. Since $E(h(x))$ is a constant, this implies that if $E(u)=0$, then $E(uh(x)=0$.\n",
    "\n",
    "Independence implies $E(g(u)x)=E(x)⋅E(g(u))$.\n",
    "When $u$ and $x$ are independent, we can express the expectation of their product $g(u)⋅x$ as: \n",
    "$E(g(u)x) = \\int\\int g(u)⋅x⋅f_{UX}(u,x) du dx$ where $f_{UX}$ is the join probability density function of $u$ and $x$. By the definition of independence, the join probability density function can be factorized as $f_{UX}(u,x)=f_{U}(u)⋅f_{X}(x)$. Substituting this into the expectation equation gives: $E(g(u)x)=\\int\\int g(u)⋅x⋅f_{U}(u)⋅f_{X}(x)dudx=(\\int g(u)⋅f_{U}(u)du⋅(\\int x⋅f_X(x)dx) = E(g(u))⋅E(x)$\n",
    "\n",
    "Therefore, under independence, the expectation of the product $g(u)⋅x$ is the product of their separate expectations, $E(x)$ and $E(g(u))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9c77f",
   "metadata": {},
   "source": [
    "## Exercise 4. Weak Instrument (Eleanor problem is correct)\n",
    "This problem explores the problem of weak instruments. The basic setup should be familiar, with\n",
    "\n",
    "$y=\\beta x+u$ <br/> $y=Z\\pi x+v$ \n",
    "\n",
    "Note that we've assumed that x is a scalar random variable, and that Z is an $l$-vector. (In general we might have k endogenous $x$ variables, so long as we have $l>k$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef10a7",
   "metadata": {},
   "source": [
    "(1) Construct a data-generating process **dgp** which takes as arguments (n; β; π) and returns a triple (y, x, Z) of n observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b8fbb",
   "metadata": {},
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "\n",
    "def data_generating_process(n, beta, pi):\n",
    "    # Generate x as a random scalar variable\n",
    "    x = np.random.normal(size=n)\n",
    "    \n",
    "    # Generate Z as an l-vector\n",
    "    l = len(pi)\n",
    "    Z = np.random.normal(size=(n, l))\n",
    "    \n",
    "    # Generate u and v as random noise\n",
    "    u = np.random.normal(size=n)\n",
    "    v = np.random.normal(size=n)\n",
    "    \n",
    "    # Generate y using the DGP equations\n",
    "    y = beta * x + u\n",
    "    y += np.dot(Z, pi) * x + v\n",
    "    \n",
    "    return y, x.reshape(-1, 1), Z\n",
    "\n",
    "# Example usage:\n",
    "n = 1000\n",
    "beta = 1\n",
    "pi = np.array([0.2, 0.3])  # Example values for pi\n",
    "y, x, Z = data_generating_process(n, beta, pi)\n",
    "\n",
    "#print(Z)\n",
    "#print(y)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b5ce9",
   "metadata": {},
   "source": [
    "(2) Use the dgp function you've constructed to explore IV (2SLS) estimates of β as a function of π when $l= 1$ using a Monte\n",
    "Carlo approach, assuming homoskedastic errors.\n",
    "\n",
    "a) Write a function two_sls which takes as arguments (y; x; Z) and returns two-stage least squares estimates of β and the standard error of the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04f759",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def two_sls(y, x, Z):\n",
    "    beta_hat1 = np.linalg.solve(Z.T @ Z, Z.T @ x)\n",
    "    x_hat = Z @ beta_hat1 \n",
    "    XtX = x_hat.T @ x_hat\n",
    "    XtY = x_hat.T @ y\n",
    "    beta_hat = np.linalg.solve(XtX, XtY)\n",
    "    e = y - x_hat @ beta_hat\n",
    "    S2_e = np.dot(e.T, e) / (x.shape[0] - x.shape[1])\n",
    "    Vb_ols = np.linalg.inv(x_hat.T @ x_hat) * S2_e\n",
    "    se = np.sqrt(np.diag(Vb_ols))\n",
    "    return beta_hat.flatten(), se.flatten()\n",
    "\n",
    "b, se = two_sls(y, x, Z)\n",
    "print(\"Estimated Coefficients (b):\\n\", b)\n",
    "print(\"Standard Errors (se):\\n\", se)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602ed56",
   "metadata": {},
   "source": [
    "(b) Taking β = π = 1, use repeated draws from dgp to check the bias, and precision of the two_sls estimator, as well as the size and power of a t-test of the hypothesis that β = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cdead",
   "metadata": {},
   "source": [
    "from scipy.stats import t as t_dist\n",
    "\n",
    "# Parameters for simulation\n",
    "n_simulations = 1000\n",
    "n_samples = 100\n",
    "beta_true = 1\n",
    "pi_true = np.array([1])\n",
    "\n",
    "# Initialize lists to store results\n",
    "bias = []\n",
    "precision = []\n",
    "t_test_results = []\n",
    "\n",
    "# Initialize coverage count\n",
    "coverage_count = 0\n",
    "\n",
    "# Run the Monte Carlo simulation\n",
    "for _ in range(n_simulations):\n",
    "    y, x, Z = data_generating_process(n_samples, beta_true, pi_true)\n",
    "    beta_hat, se = two_sls(y, x, Z)\n",
    "    \n",
    "    # Compute bias and precision\n",
    "    bias.append(np.mean(beta_hat) - beta_true)\n",
    "    precision.append(np.mean(se))\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat = beta_hat / se\n",
    "    p_value = 2 * (1 - t_dist.cdf(np.abs(t_stat), df=n_samples - x.shape[1]))\n",
    "    t_test_results.append(p_value < 0.05)  # Significance level of 0.05\n",
    "\n",
    "    # Check if true beta is within the 95% confidence interval\n",
    "    if (beta_true > beta_hat - 1.96 * se) and (beta_true < beta_hat + 1.96 * se):\n",
    "        coverage_count += 1\n",
    "        \n",
    "# Calculate coverage probability\n",
    "coverage_probability = coverage_count / n_simulations\n",
    "    \n",
    "# Aggregate results\n",
    "bias_mean = np.mean(bias)\n",
    "precision_mean = np.mean(precision)\n",
    "size = np.mean(t_test_results)\n",
    "power = 1 - size\n",
    "\n",
    "# Print results\n",
    "print(\"Bias:\", bias_mean)\n",
    "print(\"Precision (Standard Error):\", precision_mean)\n",
    "print(\"Size of t-test (Type I Error Rate):\", size)\n",
    "print(\"Power of t-test (1 - Type II Error Rate):\", power)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c41f7a",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bias, bins=30, edgecolor='black')\n",
    "plt.xlabel('Bias')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Bias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b510fe",
   "metadata": {},
   "source": [
    "Discuss. Does a 95% confidence interval (based on your2SLS estimator) correctly cover 95% of your Monte Carlo\n",
    "draws?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f4adb",
   "metadata": {},
   "source": [
    "print(\"Coverage Probability:\", coverage_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d8019",
   "metadata": {},
   "source": [
    "Based on my results, I would say it covers correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc51b68",
   "metadata": {},
   "source": [
    "Taking β = 1, but allowing $π \\in [0,1]$ again evaluate the bias and precision of the estimator, and the size and power of a t-test. The Z instrument is \"weak\" when π is \"close\" to zero. Comment on how a weak instrument affects two-stage least squares estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e5001",
   "metadata": {},
   "source": [
    "# Parameters for simulation\n",
    "# Parameters for simulation\n",
    "\n",
    "n_simulations = 1000\n",
    "n_samples = 100\n",
    "beta_true = 1\n",
    "pi_range = np.linspace(0, 1, 11)  # Vary pi from 0 to 1\n",
    "\n",
    "# Initialize arrays to store results\n",
    "bias = np.zeros(len(pi_range))\n",
    "precision = np.zeros(len(pi_range))\n",
    "size = np.zeros(len(pi_range))\n",
    "power = np.zeros(len(pi_range))\n",
    "\n",
    "# Run the Monte Carlo simulation for each pi value\n",
    "for idx, pi_value in enumerate(pi_range):\n",
    "    # Initialize counters\n",
    "    bias_sum = 0\n",
    "    se_sum = 0\n",
    "    size_count = 0\n",
    "    power_count = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        y, x, Z = data_generating_process(n_samples, beta_true, np.array([pi_value]))\n",
    "        beta_hat, se = two_sls(y, x, Z)\n",
    "        \n",
    "        # Compute bias and precision, with sum??? Ask ethan\n",
    "        bias_sum += beta_hat - beta_true\n",
    "        se_sum += se\n",
    "        \n",
    "        # Compute t-statistic and p-value for testing beta_hat = 0\n",
    "        t_stat = beta_hat / se\n",
    "        p_value = 2 * (1 - t_dist.cdf(np.abs(t_stat), df=n_samples - x.shape[1]))\n",
    "        \n",
    "        # Check if true beta is within the confidence interval\n",
    "        if (beta_true > beta_hat - 1.96 * se) and (beta_true < beta_hat + 1.96 * se):\n",
    "            size_count += 1\n",
    "        if p_value < 0.05:\n",
    "            power_count += 1\n",
    "    \n",
    "    # Calculate average bias, precision, size, and power\n",
    "    bias[idx] = bias_sum / n_simulations\n",
    "    precision[idx] = se_sum / n_simulations\n",
    "    size[idx] = size_count / n_simulations\n",
    "    power[idx] = power_count / n_simulations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4445e",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(pi_range, bias, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Bias')\n",
    "plt.title('Bias vs π')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(pi_range, precision, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Precision (Standard Error)')\n",
    "plt.title('Precision vs π')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(pi_range, size, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Size (Type I Error Rate)')\n",
    "plt.title('Size vs π')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(pi_range, power, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Power (1 - Type II Error Rate)')\n",
    "plt.title('Power vs π')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadef51b",
   "metadata": {},
   "source": [
    "(3) Now consider another \"weak\" instruments problem. Consider the sequence {1,1/2,1/4,1/8,...}. Let $l$=1,2,3,..., and for a particular value of $l$ let the vector of parameters $π_l$ consist of the first $l$ elements of the sequence. Thus, your dgp should now return Z we can treat as an n × l matrix, with successive columns of Z increasingly \"weak\" instruments.\n",
    "\n",
    "(a) Taking $\\beta = 1$, but allow l to increase (l = 1,2,...). Note that for l>1 this is now an \"overidentifed\" estimator.\n",
    "Describe the bias and precision of the estimator, and the size and power of a t-test. Compare with the case of l=1\n",
    "and π = 1.\n",
    "\n",
    "(b) What can you say about the optimal number of instruments (choice of l) in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d5d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### not suree :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c656f95",
   "metadata": {},
   "source": [
    "## 5. A Simple Approach to Inference with Weak Instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606887ed",
   "metadata": {},
   "source": [
    "Chernozhukov and C. Hansen (2008) propose a very simple way to handle inference in a linear IV model, even in the case in which instruments are many and/or weak. This problem explores the problem of weak instruments, and their method of inference. The basic setup should be identical to the above, with $y=\\beta x+u$ <br/> $x=Z \\pi+v$\n",
    "\n",
    "In this problem you will use the same dgp as in the previous problem. The idea of Chernozhukov and C. Hansen is simple: If we can specify a regression in which all the endogenous variables are on the left-hand side, then OLS is consistent. So, they subtract $ \\beta_{0}x $ from both sides of the estimating equation (for some choice of $ \\beta_{0}$), and then use the expression for x to substitute using Z, or\n",
    "\n",
    "$y-\\beta_{0}x =x(\\beta-\\beta_{0})+u$ <br/> $y-\\beta_{0}x =(Z \\pi+v)(\\beta-\\beta_{0})+u$ <br/> $y-\\beta_{0}x =(Z \\gamma)+w$ \n",
    "\n",
    "The key is that if $\\beta_{0} = \\beta$, then we will have $\\gamma = 0$. So the idea is to try to find $\\beta_{0}$ such that OLS estimates of $\\gamma$ in $y − \\beta_{0} x = Z\\gamma + w$ are close to zero.\n",
    "\n",
    "(1) Again suppose that the true $\\beta = 1$. Write a function which takes as arguments $(y, x, Z, \\beta_0)$ and which returns the p-value associated with the hypothesis that every element of $\\hat{\\gamma}$ is zero (an F-test would be appropriate). Note that this same p-value characterizes the hypothesis test that $\\beta = \\beta_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b7844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import distributions as iid\n",
    "from scipy.integrate import quad\n",
    "from scipy.linalg import inv\n",
    "\n",
    "\n",
    "# Unobservable component of instrument z\n",
    "# Can have any distribution one pleases\n",
    "w = iid.beta(1,2,loc=-iid.beta(1,2).mean()) # Centered for convenience\n",
    "\n",
    "# Structural parameters;\n",
    "sigma = {'u':1/2,'v':1/3}\n",
    "mu = {'u':2,'v':-1}\n",
    "\n",
    "# u,v assumed independent\n",
    "u = iid.norm(loc=mu['u'], scale=sigma['u'])  # Demand shocks\n",
    "v = iid.norm(loc=mu['v'], scale=sigma['v'])  # Supply shocks\n",
    "\n",
    "def weak_dgp(n, beta, pi):\n",
    "    \"\"\"\n",
    "    Generate data consistent with equations in Weak Instrument problem.\n",
    "\n",
    "    Returns a tuple with numpy arrays y, x, and Z, all of length n\n",
    "    \"\"\"\n",
    "    # Arrange u and v into matrix\n",
    "    U = np.c_[u.rvs(n), v.rvs(n)]\n",
    "    Udf = pd.DataFrame(U,columns=['u','v']) # For future reference\n",
    "    \n",
    "    # Relate Z to v\n",
    "    # Let l = 1\n",
    "#     unobserved_shock = w.rvs(n)/10\n",
    "#     Z = (1-unobserved_shock)*np.exp(4*Udf['v'] - unobserved_shock)\n",
    "    \n",
    "    # Neri: Generate Z as an l-vector\n",
    "    Z = np.random.normal(size=n)\n",
    "    \n",
    "    # Construct x\n",
    "    x = Z*pi + v.rvs(n)\n",
    "#     x = Z@pi + v.rvs(n) # if l > 1\n",
    "    \n",
    "    # Construct y\n",
    "    y = beta*x + u.rvs(n)\n",
    "    \n",
    "    # Store in DataFrame\n",
    "    df = pd.DataFrame(columns=['y', 'x', 'Z'])\n",
    "    df['y'] = y\n",
    "    df['x'] = x\n",
    "    df['Z'] = Z # note, this works because l = 1\n",
    "    return df[['y']], df[['x']], df[['Z']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79550bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "n=10000\n",
    "beta = -1\n",
    "pi = 2  \n",
    "y, x, Z =  weak_dgp(n, beta, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e3e110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          b\n",
      "0 -0.994699\n",
      "        seb\n",
      "0  0.010428\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import inv\n",
    "\n",
    "def two_sls(y, x, Z):\n",
    "    '''Takes y, x, and Z and returns a two-stage least squares estimates of beta and its standard error'''\n",
    "    # first stage, regress x on Z\n",
    "    pi_hat = np.linalg.solve(Z.T@Z,Z.T@x)\n",
    "#     pi_hat = -1*np.linalg.solve(Z.T@Z,Z.T@x) # need to multiply by -1 for some reason??\n",
    "    x_hat = Z@pi_hat\n",
    "    \n",
    "    # second stage, regress y on x-hat\n",
    "    b = np.linalg.solve(x_hat.T@x_hat, x_hat.T@y)\n",
    "    \n",
    "    # compute standard error\n",
    "    xb_df = x@b\n",
    "    xb_df = xb_df.rename(columns={0:'y'})\n",
    "    e = y - xb_df\n",
    "    s2 = (e.T@e)/(len(y)-1)\n",
    "#     vb = e.var().iloc[0]*inv(x_hat.T@x_hat) # why so different??\n",
    "    vb = s2*inv(x_hat.T@x_hat) # matches Anna's code\n",
    "    seb = np.sqrt(np.diag(vb))\n",
    "    \n",
    "    # Store in DataFrame\n",
    "    df = pd.DataFrame(columns=['b', 'seb'])\n",
    "    df['b'] = b[0]\n",
    "    df['seb'] = seb\n",
    "    return df[['b']], df[['seb']]\n",
    "\n",
    "b, seb = two_sls(y, x, Z)\n",
    "print(b)\n",
    "print(seb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2387237",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2296005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "def chernozhukov_hansen_test(y, x, Z, beta_0):\n",
    "    \n",
    "    y, x, Z =  weak_dgp(n, beta, pi)\n",
    "    a=pd.concat([x, y, Z], axis = 1, ignore_index=True)\n",
    "    x=a[0]\n",
    "    y=a[1]\n",
    "    Z=a[2]\n",
    "    \n",
    "    # Step 1: Subtract beta_0 * x from both sides\n",
    "    y_tilde = y - beta_0 * x\n",
    "    y, x, Z =  weak_dgp(n, beta, pi)\n",
    "    # Step 2: Fit the regression y_tilde = Z * gamma + w\n",
    "    gamma_hat =  np.linalg.solve(Z.T@Z,Z.T@y_tilde)\n",
    "    gamma_hat\n",
    "    \n",
    "    # Step 3: Calculate SSR for the modified model\n",
    "    e_mod = y_tilde - Z @ gamma_hat\n",
    "    SSR_mod = np.dot(e_mod.T, e_mod)\n",
    "    SSR_mod\n",
    "    \n",
    "    # Step 4: Estimate beta_hat using 2SLS\n",
    "    b, seb = two_sls(y, x, Z)  # Assuming you have the two_sls function\n",
    "    \n",
    "    y, x, Z =  weak_dgp(n, beta, pi)\n",
    "    a=pd.concat([x, y, Z], axis = 1, ignore_index=True)\n",
    "    x=a[0]\n",
    "    y=a[1]\n",
    "    Z=a[2]    \n",
    "    \n",
    "    # Step 5: Calculate SSR for the 2SLS model\n",
    "    b1 = b.iloc[0, 0]\n",
    "    e_2sls = y-x*b1\n",
    "    e_2sls\n",
    "    SSR_2sls = np.dot(e_2sls.T, e_2sls)\n",
    "    SSR_2sls\n",
    "    \n",
    "    # Step 6: Compute F-statistic\n",
    "    k = 1\n",
    "    dfn = k\n",
    "    dfd = n - k - 1\n",
    "    F_statistic = ((SSR_mod - SSR_2sls) / n - k - 1) / (SSR_2sls / k-1)\n",
    "\n",
    "    # Step 7: Calculate p-value\n",
    "    p_value = 1 - f.cdf(F_statistic, dfn, dfd)\n",
    "    p_value\n",
    "    \n",
    "    return p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d75ed0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.9800045936710555\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "beta_0 = 1  # Choose a value for beta_0\n",
    "p_value = chernozhukov_hansen_test(y, x, Z, beta_0)\n",
    "print(\"P-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18690a8",
   "metadata": {},
   "source": [
    "The p-values close to 1 indicate that we are unable to reject the null hypothesis that every element of $\\hat{\\gamma}$ is zero, or equivalently, that $\\beta = \\beta_0$. In the context of Chernozhukov and Hansen's method, this suggests that the OLS estimates of $\\gamma$ in the modified regression equation are not significantly different from zero. This aligns with their approach, where finding $\\beta_0$ such that the OLS estimates of $\\gamma$ are close to zero is a key step in handling weak instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e47639",
   "metadata": {},
   "source": [
    "(2) Using your function and taking $\\pi=1$, estimate $\\beta$ by finding the value of $\\beta_0$ which delivers maximal p-values. Describe the bias and precision of this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052364f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best beta_0: -1.9191919191919187\n",
      "Estimated beta: -1.0056377623817636\n",
      "Bias: -0.005637762381763611\n"
     ]
    }
   ],
   "source": [
    "# Define the range of beta_0 values to test\n",
    "beta_0_values = np.linspace(-10, 10, 100)  # Adjust the range as needed\n",
    "\n",
    "# Initialize variables to store results\n",
    "max_p_value = -10\n",
    "best_beta_0 = None\n",
    "\n",
    "# Iterate over beta_0 values\n",
    "for beta_0 in beta_0_values:\n",
    "    p_value = chernozhukov_hansen_test(y, x, Z, beta_0)\n",
    "    if p_value > max_p_value:\n",
    "        max_p_value = p_value\n",
    "        best_beta_0 = beta_0\n",
    "\n",
    "# Use the best beta_0 value to estimate beta\n",
    "y, x, Z = weak_dgp(n, beta, pi=1)\n",
    "b, seb = two_sls(y, x, Z)\n",
    "\n",
    "# Bias and precision\n",
    "bias = b['b'].values[0] - beta  # Difference between estimated beta and true beta\n",
    "#precision = seb['seb'].values[0]  # Standard error of the estimated beta\n",
    "\n",
    "print(\"Best beta_0:\", best_beta_0)\n",
    "print(\"Estimated beta:\", b['b'].values[0])\n",
    "print(\"Bias:\", bias)\n",
    "#print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b31b4",
   "metadata": {},
   "source": [
    "the value of Best beta_0 indicates that in the iteration over different $\\beta_0$ values, the value of $\\beta_0$ that maximized the p-value and led to the most \"insignificant\" relationship (according to the Chernozhukov and Hansen method) between the instrument $Z$ and the endogenous variable $x$ was approximately the best_beta\n",
    "\n",
    "In the context of handling weak instruments, a high p-value (close to 1) suggests that there is little evidence to reject the null hypothesis that the coefficients on the instruments are zero. This aligns with the approach of Chernozhukov and Hansen, where they aim to find a transformation of the estimating equation that makes the instrument stronger or the relationship between the instrument and the endogenous variable less influential in order to obtain reliable estimates.\n",
    "\n",
    "So, while the specific value of -5.555 may seem unusual or unexpected, it's a result of the method's attempt to \"weaken\" the influence of the instrument on the endogenous variable to address the problem of weak instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a2a51",
   "metadata": {},
   "source": [
    "(3)  Use the fact we've described about $p$-values above to construct 95% confidence intervals for your estimator of $\\beta$. Consider the coverage of this 95% confidence interval, as in the previous question. How does this compare with the 2SLS case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f357468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficient (beta_hat): -1.0055654864545616\n",
      "Standard Error (se): 0.020531001183205035\n",
      "Best beta_0: -1.9191919191919187\n",
      "Maximal p-value: 1.0\n",
      "95% Confidence Interval: (-1.0458062487736435, -0.9653247241354797)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the standard error of the estimator\n",
    "se = seb['seb'].values[0]  # Assuming seb is the standard error output from your estimation\n",
    "\n",
    "# Step 3: Compute the confidence interval based on the best beta_0\n",
    "alpha = 0.05  # Significance level for the confidence interval\n",
    "\n",
    "# Calculate the critical value using the normal distribution\n",
    "critical_value = 1.96  # For a 95% confidence level (two-sided test)\n",
    "\n",
    "# Calculate the margin of error using the standard error\n",
    "margin_of_error = critical_value * se\n",
    "\n",
    "# Calculate the estimated coefficient (beta_hat)\n",
    "y, x, Z = weak_dgp(n, beta, pi=1)  # Assuming you have the data\n",
    "b, seb = two_sls(y, x, Z)  # Assuming you have the estimation function\n",
    "beta_hat = b['b'].values[0]  # Extract the estimated coefficient\n",
    "\n",
    "# Construct the confidence interval\n",
    "lower_bound = beta_hat - margin_of_error\n",
    "upper_bound = beta_hat + margin_of_error\n",
    "\n",
    "# Print the results\n",
    "print(\"Estimated Coefficient (beta_hat):\", beta_hat)\n",
    "print(\"Standard Error (se):\", se)\n",
    "print(\"Best beta_0:\", best_beta_0)\n",
    "print(\"Maximal p-value:\", max_p_value)\n",
    "print(\"95% Confidence Interval:\", (lower_bound, upper_bound))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d4715",
   "metadata": {},
   "source": [
    "2SLS: The confidence interval in the 2SLS case is typically constructed based on the standard errors of the estimated coefficients, assuming a linear model and certain assumptions about the error terms and instrumental variables.\n",
    "\n",
    "Maximal p-value approach: The confidence interval is constructed based on the estimated coefficient $\\hat{\\beta}$ from 2SLS and the critical F-value corresponding to the desired confidence level, derived from the maximal p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4e59b",
   "metadata": {},
   "source": [
    "(4) What happens to the coverage of your test as $\\pi$ goes from 1 toward zero? How does this compare with the 2SLS case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13638d",
   "metadata": {},
   "source": [
    "As $\\pi$ decreases towards zero, the instruments become weaker. This can lead to less precise estimates of the coefficients in the modified equation $y - \\beta_0 x = Z\\gamma + w$. Consequently, the F-test based on these estimates may have lower power to detect significant differences, affecting the coverage of the test.\n",
    "\n",
    "With weaker instruments (lower $\\pi$ values), the coverage of the test based on maximizing the p-value may decrease. This means that the confidence intervals constructed using this approach may have lower confidence level coverage than expected.\n",
    "\n",
    "Weaker instruments (lower $\\pi$ values) also lead to challenges. The 2SLS estimator can become less efficient and more biased as instruments weaken, potentially impacting the coverage of confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac4b57",
   "metadata": {},
   "source": [
    "(5) Using the same construction of \"many instruments\" as in the previous question, how does the coverage of your test change as\n",
    "$l$ grows large? Again, compare with 2SLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0e507",
   "metadata": {},
   "source": [
    "With a larger $l$, we have more instruments available for the estimation. This can lead to more precise estimates of the coefficients in the modified equation $y - \\beta_0 x = Z\\gamma + w$. Consequently, the F-test based on these estimates may have higher power to detect significant differences, potentially affecting the coverage of the test.\n",
    "\n",
    "In comparison with 2sls,  in the 2SLS case with many instruments, there can be issues related to overfitting and multicollinearity. While having more instruments theoretically improves instrument strength, it can also lead to inefficiency and instability in the estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
