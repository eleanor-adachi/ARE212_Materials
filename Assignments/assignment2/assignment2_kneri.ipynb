{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a665b9b",
   "metadata": {},
   "source": [
    "## Exercise 1.3 \n",
    "\n",
    "Sometimes we will encounter estimators (e.g., Maximum likelihood) that adopt an assumption of independence, rather than mean independence. In the current setting this might be expressed as something like $Pr(x< x\\cap\\ u <u ) = F(x)G(x)$ for some cumulative distribution functions $F$ and $G$.\n",
    "\n",
    "Show that independence implies mean independence, but not the converse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd34e7",
   "metadata": {},
   "source": [
    "#### 1. Independence Implies Mean Independence\n",
    "\n",
    "Supposing two random variables $X$ and $U$, they can be expresed as:\n",
    "$Pr(X < x \\cap\\ U <u) =Pr(X<x)⋅Pr(U<u)$\n",
    "\n",
    "Suposing the mean value of a function $h(X,U)$ under this independance assumption $$E[h(X,U)] = \\int\\int h(x,u) f_x(x) f_u(u) dx du=(\\int h(x,u)f_x(x)dx)⋅(\\int h(x,u)f_u(u)du)$$\n",
    "The separation of integrals is possible due to independence. Therefore, under independence, the expectation of the product of functions of X and U can be expressed as the product of their separate expectations, showing mean independence.\n",
    "\n",
    "#### 2. Mean Independence Does Not Imply Independence\n",
    "Let's consider two random variables, X and U, with covariance $Cov(X,U)=0$ but not necessarily independent.In this case, we can have $E[XU]=E[X]⋅E[U]=0$ This means that X and U are mean independent, but they are not necessarily independent. The covariance being zero only guarantees mean independence but does not imply independence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5387c",
   "metadata": {},
   "source": [
    "## Exercise 1.4\n",
    "Related to the previous: Show that while $u$ mean independent of $x$ implies $E(uh(x)=E(u)=0$, independence also implies $E(g(u)x)=Ex Eg(u)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d4274",
   "metadata": {},
   "source": [
    "Mean independence implies $E(uh(x))=E(u)=0$. \n",
    "Given mean independence between u and x, we have $E(uh(x))=E(u)⋅E(h(x))$. Since $E(h(x))$ is a constant, this implies that if $E(u)=0$, then $E(uh(x)=0$.\n",
    "\n",
    "Independence implies $E(g(u)x)=E(x)⋅E(g(u))$.\n",
    "When $u$ and $x$ are independent, we can express the expectation of their product $g(u)⋅x$ as: \n",
    "$E(g(u)x) = \\int\\int g(u)⋅x⋅f_{UX}(u,x) du dx$ where $f_{UX}$ is the join probability density function of $u$ and $x$. By the definition of independence, the join probability density function can be factorized as $f_{UX}(u,x)=f_{U}(u)⋅f_{X}(x)$. Substituting this into the expectation equation gives: $E(g(u)x)=\\int\\int g(u)⋅x⋅f_{U}(u)⋅f_{X}(x)dudx=(\\int g(u)⋅f_{U}(u)du⋅(\\int x⋅f_X(x)dx) = E(g(u))⋅E(x)$\n",
    "\n",
    "Therefore, under independence, the expectation of the product $g(u)⋅x$ is the product of their separate expectations, $E(x)$ and $E(g(u))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725d3a0",
   "metadata": {},
   "source": [
    "## Exercise 4. Weak Instrument\n",
    "This problem explores the problem of weak instruments. The basic setup should be familiar, with\n",
    "$y=\\beta x+u$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4711f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
