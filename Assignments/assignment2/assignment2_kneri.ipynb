{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a665b9b",
   "metadata": {},
   "source": [
    "## Exercise 1.3 \n",
    "\n",
    "Sometimes we will encounter estimators (e.g., Maximum likelihood) that adopt an assumption of independence, rather than mean independence. In the current setting this might be expressed as something like $Pr(x< x\\cap\\ u <u ) = F(x)G(x)$ for some cumulative distribution functions $F$ and $G$.\n",
    "\n",
    "Show that independence implies mean independence, but not the converse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fd34e7",
   "metadata": {},
   "source": [
    "#### 1. Independence Implies Mean Independence\n",
    "\n",
    "Supposing two random variables $X$ and $U$ wich they can be expresed as:\n",
    "$Pr(X < x \\cap\\ U <u) =Pr(X<x)⋅Pr(U<u)$\n",
    "\n",
    "Suposing the mean value of a function $h(X,U)$ under this independance assumption $$E[h(X,U)] = \\int\\int h(x,u) f_x(x) f_u(u) dx du=(\\int h(x,u)f_x(x)dx)⋅(\\int h(x,u)f_u(u)du)$$\n",
    "The separation of integrals is possible due to independence. Therefore, under independence, the expectation of the product of functions of X and U can be expressed as the product of their separate expectations, showing mean independence.\n",
    "\n",
    "#### 2. Mean Independence Does Not Imply Independence\n",
    "Let's consider two random variables, X and U, with covariance $Cov(X,U)=0$ but not necessarily independent. In this case, we can have $E[XU]=E[X]⋅E[U]=0$ This means that X and U are mean independent, but they are not necessarily independent. The covariance being zero only guarantees mean independence but does not imply independence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5387c",
   "metadata": {},
   "source": [
    "## Exercise 1.4\n",
    "Related to the previous: Show that while $u$ mean independent of $x$ implies $E(uh(x)=E(u)=0$, independence also implies $E(g(u)x)=Ex Eg(u)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d4274",
   "metadata": {},
   "source": [
    "Mean independence implies $E(uh(x))=E(u)=0$. \n",
    "Given mean independence between u and x, we have $E(uh(x))=E(u)⋅E(h(x))$. Since $E(h(x))$ is a constant, this implies that if $E(u)=0$, then $E(uh(x)=0$.\n",
    "\n",
    "Independence implies $E(g(u)x)=E(x)⋅E(g(u))$.\n",
    "When $u$ and $x$ are independent, we can express the expectation of their product $g(u)⋅x$ as: \n",
    "$E(g(u)x) = \\int\\int g(u)⋅x⋅f_{UX}(u,x) du dx$ where $f_{UX}$ is the join probability density function of $u$ and $x$. By the definition of independence, the join probability density function can be factorized as $f_{UX}(u,x)=f_{U}(u)⋅f_{X}(x)$. Substituting this into the expectation equation gives: $E(g(u)x)=\\int\\int g(u)⋅x⋅f_{U}(u)⋅f_{X}(x)dudx=(\\int g(u)⋅f_{U}(u)du⋅(\\int x⋅f_X(x)dx) = E(g(u))⋅E(x)$\n",
    "\n",
    "Therefore, under independence, the expectation of the product $g(u)⋅x$ is the product of their separate expectations, $E(x)$ and $E(g(u))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9c77f",
   "metadata": {},
   "source": [
    "## Exercise 4. Weak Instrument\n",
    "This problem explores the problem of weak instruments. The basic setup should be familiar, with\n",
    "\n",
    "$y=\\beta x+u$ <br/> $y=Z\\pi x+v$ \n",
    "\n",
    "Note that we've assumed that x is a scalar random variable, and that Z is an $l$-vector. (In general we might have k endogenous $x$ variables, so long as we have $l>k$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef10a7",
   "metadata": {},
   "source": [
    "(1) Construct a data-generating process **dgp** which takes as arguments (n; β; π) and returns a triple (y, x, Z) of n observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ad1d2",
   "metadata": {},
   "source": [
    "%reset -f\n",
    "import numpy as np\n",
    "\n",
    "def data_generating_process(n, beta, pi):\n",
    "    # Generate x as a random scalar variable\n",
    "    x = np.random.normal(size=n)\n",
    "    \n",
    "    # Generate Z as an l-vector\n",
    "    l = len(pi)\n",
    "    Z = np.random.normal(size=(n, l))\n",
    "    \n",
    "    # Generate u and v as random noise\n",
    "    u = np.random.normal(size=n)\n",
    "    v = np.random.normal(size=n)\n",
    "    \n",
    "    # Generate y using the DGP equations\n",
    "    y = beta * x + u\n",
    "    y += np.dot(Z, pi) * x + v\n",
    "    \n",
    "    return y, x.reshape(-1, 1), Z\n",
    "\n",
    "# Example usage:\n",
    "n = 1000\n",
    "beta = 1\n",
    "pi = np.array([0.2, 0.3])  # Example values for pi\n",
    "y, x, Z = data_generating_process(n, beta, pi)\n",
    "\n",
    "#print(Z)\n",
    "#print(y)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b5ce9",
   "metadata": {},
   "source": [
    "(2) Use the dgp function you've constructed to explore IV (2SLS) estimates of β as a function of π when $l= 1$ using a Monte\n",
    "Carlo approach, assuming homoskedastic errors.\n",
    "\n",
    "a) Write a function two_sls which takes as arguments (y; x; Z) and returns two-stage least squares estimates of β and the standard error of the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b2dc48",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def two_sls(y, x, Z):\n",
    "    beta_hat1 = np.linalg.solve(Z.T @ Z, Z.T @ x)\n",
    "    x_hat = Z @ beta_hat1 \n",
    "    XtX = x_hat.T @ x_hat\n",
    "    XtY = x_hat.T @ y\n",
    "    beta_hat = np.linalg.solve(XtX, XtY)\n",
    "    e = y - x_hat @ beta_hat\n",
    "    S2_e = np.dot(e.T, e) / (x.shape[0] - x.shape[1])\n",
    "    Vb_ols = np.linalg.inv(x_hat.T @ x_hat) * S2_e\n",
    "    se = np.sqrt(np.diag(Vb_ols))\n",
    "    return beta_hat.flatten(), se.flatten()\n",
    "\n",
    "b, se = two_sls(y, x, Z)\n",
    "print(\"Estimated Coefficients (b):\\n\", b)\n",
    "print(\"Standard Errors (se):\\n\", se)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602ed56",
   "metadata": {},
   "source": [
    "(b) Taking β = π = 1, use repeated draws from dgp to check the bias, and precision of the two_sls estimator, as well as the size and power of a t-test of the hypothesis that β = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e978c",
   "metadata": {},
   "source": [
    "from scipy.stats import t as t_dist\n",
    "\n",
    "# Parameters for simulation\n",
    "n_simulations = 1000\n",
    "n_samples = 100\n",
    "beta_true = 1\n",
    "pi_true = np.array([1])\n",
    "\n",
    "# Initialize lists to store results\n",
    "bias = []\n",
    "precision = []\n",
    "t_test_results = []\n",
    "\n",
    "# Initialize coverage count\n",
    "coverage_count = 0\n",
    "\n",
    "# Run the Monte Carlo simulation\n",
    "for _ in range(n_simulations):\n",
    "    y, x, Z = data_generating_process(n_samples, beta_true, pi_true)\n",
    "    beta_hat, se = two_sls(y, x, Z)\n",
    "    \n",
    "    # Compute bias and precision\n",
    "    bias.append(np.mean(beta_hat) - beta_true)\n",
    "    precision.append(np.mean(se))\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat = beta_hat / se\n",
    "    p_value = 2 * (1 - t_dist.cdf(np.abs(t_stat), df=n_samples - x.shape[1]))\n",
    "    t_test_results.append(p_value < 0.05)  # Significance level of 0.05\n",
    "\n",
    "    # Check if true beta is within the 95% confidence interval\n",
    "    if (beta_true > beta_hat - 1.96 * se) and (beta_true < beta_hat + 1.96 * se):\n",
    "        coverage_count += 1\n",
    "        \n",
    "# Calculate coverage probability\n",
    "coverage_probability = coverage_count / n_simulations\n",
    "    \n",
    "# Aggregate results\n",
    "bias_mean = np.mean(bias)\n",
    "precision_mean = np.mean(precision)\n",
    "size = np.mean(t_test_results)\n",
    "power = 1 - size\n",
    "\n",
    "# Print results\n",
    "print(\"Bias:\", bias_mean)\n",
    "print(\"Precision (Standard Error):\", precision_mean)\n",
    "print(\"Size of t-test (Type I Error Rate):\", size)\n",
    "print(\"Power of t-test (1 - Type II Error Rate):\", power)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07da4b",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(bias, bins=30, edgecolor='black')\n",
    "plt.xlabel('Bias')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Bias')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b510fe",
   "metadata": {},
   "source": [
    "Discuss. Does a 95% confidence interval (based on your2SLS estimator) correctly cover 95% of your Monte Carlo\n",
    "draws?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c8c6a",
   "metadata": {},
   "source": [
    "print(\"Coverage Probability:\", coverage_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d8019",
   "metadata": {},
   "source": [
    "Based on my results, I would say it covers correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc51b68",
   "metadata": {},
   "source": [
    "Taking β = 1, but allowing $π \\in [0,1]$ again evaluate the bias and precision of the estimator, and the size and power of a t-test. The Z instrument is \"weak\" when π is \"close\" to zero. Comment on how a weak instrument affects two-stage least squares estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036b386",
   "metadata": {},
   "source": [
    "# Parameters for simulation\n",
    "# Parameters for simulation\n",
    "\n",
    "n_simulations = 1000\n",
    "n_samples = 100\n",
    "beta_true = 1\n",
    "pi_range = np.linspace(0, 1, 11)  # Vary pi from 0 to 1\n",
    "\n",
    "# Initialize arrays to store results\n",
    "bias = np.zeros(len(pi_range))\n",
    "precision = np.zeros(len(pi_range))\n",
    "size = np.zeros(len(pi_range))\n",
    "power = np.zeros(len(pi_range))\n",
    "\n",
    "# Run the Monte Carlo simulation for each pi value\n",
    "for idx, pi_value in enumerate(pi_range):\n",
    "    # Initialize counters\n",
    "    bias_sum = 0\n",
    "    se_sum = 0\n",
    "    size_count = 0\n",
    "    power_count = 0\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        y, x, Z = data_generating_process(n_samples, beta_true, np.array([pi_value]))\n",
    "        beta_hat, se = two_sls(y, x, Z)\n",
    "        \n",
    "        # Compute bias and precision, with sum??? Ask ethan\n",
    "        bias_sum += beta_hat - beta_true\n",
    "        se_sum += se\n",
    "        \n",
    "        # Compute t-statistic and p-value for testing beta_hat = 0\n",
    "        t_stat = beta_hat / se\n",
    "        p_value = 2 * (1 - t_dist.cdf(np.abs(t_stat), df=n_samples - x.shape[1]))\n",
    "        \n",
    "        # Check if true beta is within the confidence interval\n",
    "        if (beta_true > beta_hat - 1.96 * se) and (beta_true < beta_hat + 1.96 * se):\n",
    "            size_count += 1\n",
    "        if p_value < 0.05:\n",
    "            power_count += 1\n",
    "    \n",
    "    # Calculate average bias, precision, size, and power\n",
    "    bias[idx] = bias_sum / n_simulations\n",
    "    precision[idx] = se_sum / n_simulations\n",
    "    size[idx] = size_count / n_simulations\n",
    "    power[idx] = power_count / n_simulations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f2013",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(pi_range, bias, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Bias')\n",
    "plt.title('Bias vs π')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(pi_range, precision, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Precision (Standard Error)')\n",
    "plt.title('Precision vs π')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(pi_range, size, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Size (Type I Error Rate)')\n",
    "plt.title('Size vs π')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(pi_range, power, marker='o')\n",
    "plt.xlabel('π')\n",
    "plt.ylabel('Power (1 - Type II Error Rate)')\n",
    "plt.title('Power vs π')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadef51b",
   "metadata": {},
   "source": [
    "(3) Now consider another \"weak\" instruments problem. Consider the sequence {1,1/2,1/4,1/8,...}. Let $l$=1,2,3,..., and for a particular value of $l$ let the vector of parameters $π_l$ consist of the first $l$ elements of the sequence. Thus, your dgp should now return Z we can treat as an n × l matrix, with successive columns of Z increasingly \"weak\" instruments.\n",
    "\n",
    "(a) Taking $\\beta = 1$, but allow l to increase (l = 1,2,...). Note that for l>1 this is now an \"overidentifed\" estimator.\n",
    "Describe the bias and precision of the estimator, and the size and power of a t-test. Compare with the case of l=1\n",
    "and π = 1.\n",
    "\n",
    "(b) What can you say about the optimal number of instruments (choice of l) in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d5d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "### not suree :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c656f95",
   "metadata": {},
   "source": [
    "## 5. A Simple Approach to Inference with Weak Instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606887ed",
   "metadata": {},
   "source": [
    "Chernozhukov and C. Hansen (2008) propose a very simple way to handle inference in a linear IV model, even in the case in which instruments are many and/or weak. This problem explores the problem of weak instruments, and their method of inference. The basic setup should be identical to the above, with $y=\\beta x+u$ <br/> $x=Z \\pi+v$\n",
    "\n",
    "In this problem you will use the same dgp as in the previous problem. The idea of Chernozhukov and C. Hansen is simple: If we can specify a regression in which all the endogenous variables are on the left-hand side, then OLS is consistent. So, they subtract $ \\beta_{0}x $ from both sides of the estimating equation (for some choice of $ \\beta_{0}$), and then use the expression for x to substitute using Z, or\n",
    "\n",
    "$y-\\beta_{0}x =x(\\beta-\\beta_{0})+u$ <br/> $y-\\beta_{0}x =(Z \\pi+v)(\\beta-\\beta_{0})+u$ <br/> $y-\\beta_{0}x =(Z \\gamma)+w$ \n",
    "\n",
    "The key is that if $\\beta_{0} = \\beta$, then we will have $\\gamma = 0$. So the idea is to try to find $\\beta_{0}$ such that OLS estimates of $\\gamma$ in $y − \\beta_{0} x = Z\\gamma + w$ are close to zero.\n",
    "\n",
    "(1) Again suppose that the true $\\beta = 1$. Write a function which takes as arguments $(y, x, Z, \\beta_0)$ and which returns the p-value associated with the hypothesis that every element of $\\hat{\\delta}$ is zero (an F-test would be appropriate). Note that this same p-value characterizes the hypothesis test that $\\beta = \\beta_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4834c40f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12588\\1309298083.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Assuming y, x, and Z are already defined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mbeta_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# Choose a value for beta_0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mp_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;31m#p_value = 1 - scipy.stats.f.cdf(F, df1, df2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"P-value:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "def f_test(y, x, Z, beta_0):\n",
    "    # Step 1: Estimate gamma_hat using OLS on the modified equation\n",
    "    y_mod = y - beta_0 * x\n",
    "    X_mod = Z\n",
    "    gamma_hat = np.linalg.lstsq(X_mod, y_mod, rcond=None)[0]\n",
    "    \n",
    "    # Step 2: Calculate SSR for the modified model\n",
    "    e_mod = y_mod - X_mod @ gamma_hat\n",
    "    SSR_mod = np.dot(e_mod.T, e_mod)\n",
    "    \n",
    "    # Step 3: Estimate beta_hat using 2SLS\n",
    "    beta_hat, _ = two_sls(y, x, Z)\n",
    "    \n",
    "    # Step 4: Calculate SSR for the 2SLS model\n",
    "    e_2sls = y - x @ beta_hat\n",
    "    SSR_2sls = np.dot(e_2sls.T, e_2sls)\n",
    "    \n",
    "    # Step 5: Compute F-statistic\n",
    "    n = y.shape[0]\n",
    "    k = Z.shape[1]\n",
    "    dfn = k\n",
    "    dfd = n - k - 1\n",
    "    F_statistic = ((SSR_mod - SSR_2sls) / dfn) / (SSR_2sls / dfd)\n",
    "    \n",
    "    # Step 6: Calculate p-value\n",
    "    p_value = 1 - f.cdf(F_statistic, dfn, dfd)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "# Example usage:\n",
    "# Assuming y, x, and Z are already defined\n",
    "beta_0 = 1  # Choose a value for beta_0\n",
    "p_value = scipy.stats.f.cdf(y, x, Z, beta_0)\n",
    "#p_value = 1 - scipy.stats.f.cdf(F, df1, df2)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"F_statistic\",F_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18690a8",
   "metadata": {},
   "source": [
    "We notice all the p-values for the F-test are 1. This means that for each iteration or simulation, the F-test does not reject the null hypothesis that every element of $\\hat{\\delta}$ is zero. In other words, it suggests that there is no significant evidence to suggest that $\\beta = \\beta_0$ for the chosen value of $\\beta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e47639",
   "metadata": {},
   "source": [
    "(2) Using your function and taking $\\pi=1$, estimate $\\beta$ by finding the value of $\\beta_0$ which delivers maximal p-values. Describe the bias and precision of this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b834598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best beta_0: 0.0\n",
      "Maximal p-value: 1.0\n"
     ]
    }
   ],
   "source": [
    "def find_max_p_value(y, x, Z, beta_range):\n",
    "    max_p_value = 0\n",
    "    best_beta_0 = None\n",
    "    \n",
    "    for beta_0 in beta_range:\n",
    "        p_value = f_test(y, x, Z, beta_0)\n",
    "        if np.max(p_value) > max_p_value:  # Compare maximum p-value\n",
    "            max_p_value = np.max(p_value)\n",
    "            best_beta_0 = beta_0\n",
    "    \n",
    "    return best_beta_0, max_p_value\n",
    "\n",
    "# Example usage:\n",
    "# Assuming y, x, and Z are already defined\n",
    "beta_range = np.linspace(0, 2, 1000)  # Range of beta_0 values to search\n",
    "best_beta_0, max_p_value = find_max_p_value(y, x, Z, beta_range)\n",
    "print(\"Best beta_0:\", best_beta_0)\n",
    "print(\"Maximal p-value:\", max_p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b31b4",
   "metadata": {},
   "source": [
    "The result means that according to the method used (Chernozhukov and Hansen's approach), the best estimate for $\\beta_0$ that maximizes the p-value (which is 1.0, indicating no statistical significance) is 0.0.\n",
    "In other words, the hypothesis that every element of $\\hat{\\delta}$ is zero (which corresponds to $\\beta = \\beta_0$ in this context) is not rejected based on the given data and model. This suggests that there is not enough evidence to conclude that $\\beta$ differs significantly from $\\beta_0$, where $\\beta_0$ is chosen to be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a2a51",
   "metadata": {},
   "source": [
    "(3)  Use the fact we've described about $p$-values above to construct 95% confidence intervals for your estimator of $\\beta$. Consider the coverage of this 95% confidence interval, as in the previous question. How does this compare with the 2SLS case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f357468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Coefficient (beta_hat): [2.85011539]\n",
      "Standard Error (se): [2.01595056]\n",
      "Best beta_0: 0.0\n",
      "Maximal p-value: 1.0\n",
      "95% Confidence Interval: [array([-4.91287478]), array([10.61310556])]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Compute the confidence interval based on the best beta_0\n",
    "alpha = 0.05  # Significance level for the confidence interval\n",
    "critical_value = f.ppf(1 - alpha, dfn=1, dfd=n - 1)  # Calculate critical F-value\n",
    "confidence_interval = [beta_hat - critical_value * se, beta_hat + critical_value * se]\n",
    "\n",
    "# Print the results\n",
    "print(\"Estimated Coefficient (beta_hat):\", beta_hat)\n",
    "print(\"Standard Error (se):\", se)\n",
    "print(\"Best beta_0:\", best_beta_0)\n",
    "print(\"Maximal p-value:\", max_p_value)\n",
    "print(\"95% Confidence Interval:\", confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d4715",
   "metadata": {},
   "source": [
    "2SLS: The confidence interval in the 2SLS case is typically constructed based on the standard errors of the estimated coefficients, assuming a linear model and certain assumptions about the error terms and instrumental variables.\n",
    "\n",
    "Maximal p-value approach: The confidence interval is constructed based on the estimated coefficient $\\hat{\\beta}$ from 2SLS and the critical F-value corresponding to the desired confidence level, derived from the maximal p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4e59b",
   "metadata": {},
   "source": [
    "(4) What happens to the coverage of your test as $\\pi$ goes from 1 toward zero? How does this compare with the 2SLS case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13638d",
   "metadata": {},
   "source": [
    "As $\\pi$ decreases towards zero, the instruments become weaker. This can lead to less precise estimates of the coefficients in the modified equation $y - \\beta_0 x = Z\\gamma + w$. Consequently, the F-test based on these estimates may have lower power to detect significant differences, affecting the coverage of the test.\n",
    "\n",
    "With weaker instruments (lower $\\pi$ values), the coverage of the test based on maximizing the p-value may decrease. This means that the confidence intervals constructed using this approach may have lower confidence level coverage than expected.\n",
    "\n",
    "Weaker instruments (lower $\\pi$ values) also lead to challenges. The 2SLS estimator can become less efficient and more biased as instruments weaken, potentially impacting the coverage of confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac4b57",
   "metadata": {},
   "source": [
    "(5) Using the same construction of \"many instruments\" as in the previous question, how does the coverage of your test change as\n",
    "$l$ grows large? Again, compare with 2SLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0e507",
   "metadata": {},
   "source": [
    "With a larger $l$, we have more instruments available for the estimation. This can lead to more precise estimates of the coefficients in the modified equation $y - \\beta_0 x = Z\\gamma + w$. Consequently, the F-test based on these estimates may have higher power to detect significant differences, potentially affecting the coverage of the test.\n",
    "\n",
    "In comparison with 2sls,  in the 2SLS case with many instruments, there can be issues related to overfitting and multicollinearity. While having more instruments theoretically improves instrument strength, it can also lead to inefficiency and instability in the estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
