{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99c228f",
   "metadata": {},
   "source": [
    "## 4. Tests of Normality\n",
    "Suppose we have a sample of iid observations $x_1, x_2, . . . , x_N$; we want to test whether these are drawn from a normal distribution. Note the fact that the integer central moments of the normal distribution satisfy \n",
    "\n",
    "$Ex=\\mu$ \n",
    "\n",
    "$E(x-\\mu)^m=0$,   $\\ \\ \\ m$  odd \n",
    "\n",
    "$E(x-\\mu)^m=\\sigma^m(m-1)$!!  $\\ \\ \\ m$ even\n",
    "\n",
    "where n!! is the double factorial, i.e., $n!!=n(n-2)(n-4)...$\n",
    "#### 1. Using the analogy principle, construct an estimator for the first $k$ moments of the distribution of $x$. Use this to define a k-vector of moment restrictions $ g_N(\\mu, \\sigma) $ satisfying $E_{gN}(\\mu, \\sigma)=0$ under the null hypothesis of normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc060498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.027109073490359778, 0.0, 0.04688252785024593, -1.039938328364765, -0.5579449467073471]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import factorial2\n",
    "\n",
    "#Define the sample moments function, mean-var-kurtosis etc\n",
    "def sample_moments(data, k):\n",
    "    N = len(data)\n",
    "    mu_hat = np.mean(data)\n",
    "    moments = [mu_hat]  # Start with the sample mean\n",
    "    \n",
    "    for m in range(2, k+1):\n",
    "        central_moment = np.mean((data - mu_hat)**m)\n",
    "        moments.append(central_moment)\n",
    "    \n",
    "    return moments\n",
    "\n",
    "def moment_restrictions(data, k):\n",
    "    moments = sample_moments(data, k)\n",
    "    restrictions = []\n",
    "    \n",
    "    for m in range(1, k+1):\n",
    "        if m % 2 == 1:  # Odd moments\n",
    "            restrictions.append(moments[m-1])  # Should be approximately zero\n",
    "        else:  # Even moments\n",
    "            sigma_hat = np.sqrt(moments[1])  # Sample variance (second central moment)\n",
    "            theoretical_moment = sigma_hat**m * factorial2(m-1)\n",
    "            restrictions.append(moments[m-1] - theoretical_moment)\n",
    "    \n",
    "    return restrictions\n",
    "\n",
    "# Example usage\n",
    "np.random.seed(123)\n",
    "data = np.random.normal(0, 1, 100)  # Sample data from a normal distribution\n",
    "k = 5  # Number of moments to consider\n",
    "print(moment_restrictions(data, k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c541e7",
   "metadata": {},
   "source": [
    "#### 2. What is the covariance matrix of the sample moment restrictions (again under the null)? I.e., what can be said about $Eg_{j}(\\mu, \\sigma) g_j(\\mu, \\sigma)^T - Eg_{j}(\\mu, \\sigma) Eg_j(\\mu, \\sigma)^T$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b25e1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e3ddd36",
   "metadata": {},
   "source": [
    " $Eg_{j}(\\mu, \\sigma) g_j(\\mu, \\sigma)^T$ --- This term represents the expected value of the outer product of the vector of moment restrictions with itself. This results in a matrix where each element is the expected product of two moment conditions.\n",
    " \n",
    " $Eg_{j}(\\mu, \\sigma) Eg_j(\\mu, \\sigma)^T$ --- Since under the null hypothesis $Eg_j(\\mu, \\sigma)=0$ (because each is $g_{N}$  is constructed to be zero when the data follow a normal distribution), this term simplifies to a matrix of zeros.\n",
    "\n",
    "Thus, the expression simplifies to: $Cov[g_{j}(\\mu, \\sigma)]=Eg_{j}(\\mu, \\sigma) g_j(\\mu, \\sigma)^T$ because $Eg_{j}(\\mu, \\sigma) Eg_j(\\mu, \\sigma)^T$ is a matrix of zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caff08e",
   "metadata": {},
   "source": [
    "#### 3. Using your answers to the previous two questions, suggest a GMM-based test of the hypothesis of normality, taking $k > 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4302d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 329.1660952670244, Critical value: 5.991464547107979\n",
      "Reject the null hypothesis of normality.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bootstrap_covariance(data, moment_func, k, B=1000):\n",
    "    \"\"\"\n",
    "    Estimate the covariance matrix of moment restrictions using bootstrap.\n",
    "    \n",
    "    Parameters:\n",
    "        data (array): The original dataset.\n",
    "        moment_func (function): A function to calculate the moment restrictions.\n",
    "        k (int): The number of moment restrictions.\n",
    "        B (int): The number of bootstrap samples.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Estimated covariance matrix of the moment restrictions.\n",
    "    \"\"\"\n",
    "    N = len(data)\n",
    "    bootstrap_samples = np.random.choice(data, (B, N), replace=True)\n",
    "    moment_samples = np.array([moment_func(sample, k) for sample in bootstrap_samples])\n",
    "    mean_moments = np.mean(moment_samples, axis=0)\n",
    "    cov_matrix = np.cov(moment_samples, rowvar=False)\n",
    "    \n",
    "    return cov_matrix\n",
    "\n",
    "def gmm_test_normality(data, k):\n",
    "    N = len(data)\n",
    "    mu_hat = np.mean(data)\n",
    "    sigma_hat = np.std(data, ddof=1)\n",
    "    \n",
    "    # Ensure gN_vector is an array\n",
    "    gN_vector = np.array(moment_restrictions(data, k))\n",
    "    cov_gN = bootstrap_covariance(data, moment_restrictions, k)  # Simulated covariance matrix\n",
    "    \n",
    "    W_N = np.linalg.inv(cov_gN)  # Weighting matrix\n",
    "    J_N = N * np.dot(np.dot(gN_vector.T, W_N), gN_vector)  # GMM objective function\n",
    "    \n",
    "    chi2_critical = chi2.ppf(0.95, df=k-2)  # Chi-square critical value\n",
    "    \n",
    "    print(f\"Test statistic: {J_N}, Critical value: {chi2_critical}\")\n",
    "    if J_N > chi2_critical:\n",
    "        print(\"Reject the null hypothesis of normality.\")\n",
    "    else:\n",
    "        print(\"Do not reject the null hypothesis of normality.\")\n",
    "\n",
    "# Assuming data and k are defined and the moment_restrictions function is implemented properly\n",
    "# Example:\n",
    "data = np.random.normal(0, 1, size=100)\n",
    "gmm_test_normality(data, k=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e738d",
   "metadata": {},
   "source": [
    "#### 4. Implement the test you’ve devised using python. You may want to use scipy.stats.distributions.chi2.cdf and scipy.optimize.minimize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f7c0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from scipy.special import factorial2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37b77002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moment_restrictions(data, params):\n",
    "    mu, sigma = params\n",
    "    k = 4  # Number of moments to use, can be adjusted\n",
    "    moments = []\n",
    "    for m in range(1, k + 1):\n",
    "        if m % 2 == 1:  # Odd moments\n",
    "            moment = np.mean((data - mu) ** m)\n",
    "        else:  # Even moments\n",
    "            moment = np.mean((data - mu) ** m) - sigma**m * factorial2(m - 1)\n",
    "        moments.append(moment)\n",
    "    return np.array(moments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0d47560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_covariance(data, func, params, B=1000):\n",
    "    N = len(data)\n",
    "    bootstraps = [func(data[np.random.choice(N, N, replace=True)], params) for _ in range(B)]\n",
    "    bootstrap_matrix = np.array(bootstraps)\n",
    "    return np.cov(bootstrap_matrix, rowvar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4031552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_objective(params, data, W):\n",
    "    g = moment_restrictions(data, params)\n",
    "    return g.T @ W @ g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c881d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic: 21.12051636456306\n",
      "P-Value: 2.5926156839939374e-05\n",
      "Estimated Parameters (mu, sigma): [-0.08592015  0.99730187]\n"
     ]
    }
   ],
   "source": [
    "def gmm_test_normality(data):\n",
    "    # Initial parameter guesses\n",
    "    initial_params = [np.mean(data), np.std(data)]\n",
    "    \n",
    "    # Estimating the covariance matrix with initial parameters\n",
    "    W_initial = np.linalg.inv(bootstrap_covariance(data, moment_restrictions, initial_params))\n",
    "    \n",
    "    # Minimize the GMM objective function\n",
    "    result = optimize.minimize(gmm_objective, initial_params, args=(data, W_initial))\n",
    "    \n",
    "    # Best fit parameters\n",
    "    best_params = result.x\n",
    "    \n",
    "    # Recalculate W with best fit parameters\n",
    "    W = np.linalg.inv(bootstrap_covariance(data, moment_restrictions, best_params))\n",
    "    \n",
    "    # Calculate the GMM objective function with best fit parameters\n",
    "    J = gmm_objective(best_params, data, W)\n",
    "    test_statistic = len(data) * J\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = len(moment_restrictions(data, best_params)) - len(best_params)\n",
    "    \n",
    "    # p-value from chi-square distribution\n",
    "    p_value = 1 - stats.chi2.cdf(test_statistic, df)\n",
    "    \n",
    "    return test_statistic, p_value, best_params\n",
    "\n",
    "# Test the function with sample data\n",
    "data = np.random.normal(0, 1, size=100)\n",
    "test_statistic, p_value, params = gmm_test_normality(data)\n",
    "print(\"Test Statistic:\", test_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "print(\"Estimated Parameters (mu, sigma):\", params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ea5df",
   "metadata": {},
   "source": [
    "#### 5.  What can be said about the optimal choice of k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f0e12",
   "metadata": {},
   "source": [
    "The optimal choice of $𝑘$, the number of moment conditions used in a GMM approach, is crucial and can significantly influence the test's power and size.  \n",
    "- When the number of moment conditions exceeds the number of parameters being estimated, the model becomes over-identified. \n",
    "- Using more moment conditions can potentially make the estimator more efficient because it utilizes more information about the distribution. This is generally true up to a point where the added moments bring in relevant additional information.\n",
    "- For a normality test, including higher than the second moments is important because the normal distribution is completely characterized by its first two moments (mean and variance). Moments higher than the second (skewness, kurtosis, etc.) can provide crucial information about departures from normality (e.g., asymmetry, tail heaviness).\n",
    "- As $𝑘$ increases, the moments might become increasingly difficult to estimate accurately, especially for small sample sizes. Moments of higher order are also more sensitive to outliers and can become numerically unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc29554",
   "metadata": {},
   "source": [
    "Compare the GMM estimates of $(\\mu, \\sigma)$ to the maximum likelihood estimates of these parameters. Do they differ? Why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830d9f4",
   "metadata": {},
   "source": [
    "- MLE directly aims to find the parameter values that maximize the likelihood function given the data. For the normal distribution, MLE for $(\\mu, \\sigma)$ are the sample mean and sample standard deviation, respectively, making it very efficient (in the statistical sense) and unbiased in large samples.\n",
    "- MLE is known for being asymptotically efficient for many models, including the normal distribution. This means it achieves the lowest possible variance among all unbiased estimators as the sample size grows.\n",
    "- GMM is based on the method of moments, where parameters are chosen so that the sample moments (often beyond the first two) match their theoretical counterparts as closely as possible. This can be particularly useful when multiple moment conditions are involved, or the distributional assumptions for MLE are not fully justified.\n",
    "- GMM can be more robust to certain specification errors or when dealing with distributions where MLE is difficult to implement or unreliable due to model complexity or constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b4907",
   "metadata": {},
   "source": [
    "- For the normal distribution, if the model is correctly specified and the sample size is large, GMM and MLE for $(\\mu, \\sigma)$ should be quite similar, especially if the GMM uses the first and second moments as part of its conditions.\n",
    "- If there are deviations from normality or if additional moments (like skewness and kurtosis) are incorporated into the GMM, the estimates might differ. GMM might then provide an estimate that compensates for these higher-order characteristics, whereas MLE would strictly interpret parameters under the assumption of normality.\n",
    "- In small samples, MLE can be biased (particularly the estimate for $(\\sigma)$, whereas GMM might also suffer from high variability depending on the moments used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c613a16",
   "metadata": {},
   "source": [
    "## 5. Logit\n",
    "\n",
    "This problem is meant to help draw connections between GMM estimators and maximum likelihood estimators, with a particular focus on the ’logit’ model. The development of a maximum likelihood estimator typically begins with an assumption that some random variable has a (conditional) distribution which is known up a k-vector of parameters $\\beta$. \n",
    "Consider the case in which we observe N independent realizations of a Bernoulli random variable $Y$, with $Pr(Y=1|X)=\\sigma(\\beta^TX)$ and $Pr(Y=0|X)=1-\\sigma(\\beta^TX)$\n",
    "\n",
    "#### 1. Show that under this model $E(Y_i -\\sigma(X\\beta)|X)=0$. Assume that $\\sigma$ is a known function, and use this fact to develop a GMM estimator of $\\beta$. Is your estimator just- or over-identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b6fe5",
   "metadata": {},
   "source": [
    "Under the Model\n",
    "In the logistic regression model (logit model), the probability of observing $Y = 1$ given predictors $X$ is modeled as:\n",
    "${Pr}(Y = 1 | X) = \\sigma(\\beta^T X)$\n",
    "where $\\sigma$ is the logistic function defined by:\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "Conversely, the probability of observing $Y = 0$ given $X$ is:\n",
    "${Pr}(Y = 0 | X) = 1 - \\sigma(\\beta^T X)$\n",
    "\n",
    "Establishing the Expectation\n",
    "To show that $E(Y_i - \\sigma(X\\beta) | X) = 0$. To see why this is true, we need to consider the definition of expectation for a Bernoulli random variable $Y$:\n",
    "$E(Y | X) = 1 \\cdot \\text{Pr}(Y = 1 | X) + 0 \\cdot \\text{Pr}(Y = 0 | X) = \\sigma(\\beta^T X)$\n",
    "\n",
    "Therefore, the expectation of the difference $Y - \\sigma(\\beta^T X)$ given $X$ is:\n",
    "\n",
    "$E(Y - \\sigma(\\beta^T X) | X) = E(Y | X) - \\sigma(\\beta^T X) = \\sigma(\\beta^T X) - \\sigma(\\beta^T X) = 0$\n",
    "\n",
    "This shows that the expectation condition holds under the logistic model.\n",
    "\n",
    "Given the expectation condition $E(Y_i - \\sigma(X_i \\beta) | X_i) = 0$, we can utilize this as the basis for a GMM estimator. The moment conditions for the GMM are:\n",
    "$g(\\beta, X, Y) = (Y - \\sigma(\\beta^T X))X$\n",
    "\n",
    "The GMM estimator will aim to find $\\beta$ such that the sample analogue of the expectation of $g(\\beta, X, Y)$ is zero:\n",
    "\n",
    "$\\frac{1}{N} \\sum_{i=1}^N (Y_i - \\sigma(\\beta^T X_i)) X_i = 0$\n",
    "\n",
    "Just- or Over-Identified?\n",
    "\n",
    "Whether the estimator is just- or over-identified depends on the number of parameters to estimate $\\beta$ and the number of moment conditions available.\n",
    "\n",
    "Just- or Over-Identified?\n",
    "Whether the estimator is just- or over-identified depends on the number of parameters to estimate $\\beta$ and the number of moment conditions available.\n",
    "\n",
    "- **Just-Identified**: If the number of parameters in $\\beta$ is equal to the number of equations (i.e., the dimension of $X$, the system is just-identified. This typically implies that there is exactly one moment condition per parameter to estimate.\n",
    "- **Over-Identified**: If there are more moment conditions than the number of parameters in $\\beta$ (for example, if $X$ includes multiple variables or instruments beyond the number of parameters), the system is over-identified. This can potentially provide a test for the model's validity through the over-identifying restrictions test.\n",
    "\n",
    "In many practical cases, especially if $X$ is multi-dimensional and $\\beta$ includes intercepts or multiple regression coefficients, the system can be just- or over-identified depending on how $X$ and $\\beta$ are specified. The GMM estimation approach will use weighting matrices (typically the inverse of the covariance matrix of the moment conditions) to weight the moment conditions and solve for $\\beta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47bef9",
   "metadata": {},
   "source": [
    "#### 2. Show that the likelihood can be written as $L(\\beta|y,X) = \\prod_{i=1}^N\\sigma(\\beta^T X_i)^{y_i}\\left(1-\\sigma(\\beta^T X_i)\\right)^{1-y_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220a9ef",
   "metadata": {},
   "source": [
    "* Model Probabilities\n",
    "\n",
    "In logistic regression, the probability that $ Y = 1 $ given predictors $ X $ is modeled by the logistic function:\n",
    "${Pr}(Y = 1 | X) = \\sigma(\\beta^T X) $\n",
    "\n",
    "where $ \\sigma(z) $ is the logistic function defined as:\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "Conversely, the probability that $ Y = 0 $ given $ X $ is the complement:\n",
    "${Pr}(Y = 0 | X) = 1 - \\sigma(\\beta^T X) $\n",
    "\n",
    "* Likelihood Function\n",
    "\n",
    "The likelihood function for a logistic regression model, given $ N $ independent observations, is the product of the probabilities of observing each data point $ i $ given the parameters $ \\beta $. For each observation $ i $, the likelihood contribution depends on whether $ Y_i = 1 $ or $ Y_i = 0 $.\n",
    "\n",
    "The individual contribution to the likelihood for each data point can be expressed as:\n",
    "${Pr}(Y_i = y_i | X_i) = \\sigma(\\beta^T X_i)^{y_i} \\left(1 - \\sigma(\\beta^T X_i)\\right)^{1 - y_i} $\n",
    "\n",
    "where $ y_i $ is the observed value of $ Y $ for observation $ i $ (either 0 or 1).\n",
    "\n",
    "* Complete Likelihood Function\n",
    "\n",
    "Given $ N $ independent observations, the complete likelihood function is the product of all individual likelihoods:\n",
    "\n",
    "$L(\\beta | y, X) = \\prod_{i=1}^N \\sigma(\\beta^T X_i)^{y_i} \\left(1 - \\sigma(\\beta^T X_i)\\right)^{1 - y_i} $\n",
    "\n",
    "* Explanation\n",
    "\n",
    "- The term $ \\sigma(\\beta^T X_i)^{y_i} $ represents the likelihood of observing $ Y_i = 1 $ if $ y_i = 1 $ and contributes 1 (i.e., no effect) to the product if $ y_i = 0 $.\n",
    "- The term $ \\left(1 - \\sigma(\\beta^T X_i)\\right)^{1 - y_i} $ represents the likelihood of observing $ Y_i = 0 $ if $ y_i = 0 $ and contributes 1 if $ y_i = 1 $.\n",
    "\n",
    "This formulation reflects the binary nature of the outcome variable $ Y $ and appropriately weights the likelihood contributions depending on the observed values of $ Y $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd01bc",
   "metadata": {},
   "source": [
    "#### 3. To obtain the maximum likelihood estimator (MLE) one can chose $b$ to maximize $\\log L(b|y,X)$. When the likelihood is well-behaved, the MLE estimator satisfies the first order conditions (also called the \"scores'') from this maximization problem, in which case this is called a \"type I'' MLE.  Let $\\sigma(z)=\\frac{1}{1+e^{-z}}$ (this is sometimes called the logistic function, or the sigmoid function), and obtain the scores $S_N(b)$ for this estimation problem.  Show that $E S_N(\\beta) = 0$.  Demonstrate that these moment conditions can serve as the basis for a GMM estimator of \\(\\beta\\), and compare this estimator to the GMM estimator you developed above.  Which is more efficient, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdb7ec",
   "metadata": {},
   "source": [
    "* Derivation of Scores for MLE\n",
    "\n",
    "In the logistic regression context, the log likelihood function $ log L(b|y,X) $ for parameters $ b $ given binary outcomes $ y $ and predictors $ X $ is the natural logarithm of the likelihood function $ L(b|y,X) $. The likelihood function itself was established as:\n",
    "\n",
    "$ L(b | y, X) = \\prod_{i=1}^N \\sigma(b^T X_i)^{y_i} \\left(1 - \\sigma(b^T X_i)\\right)^{1 - y_i} $\n",
    "\n",
    "Thus, the log likelihood becomes:\n",
    "\n",
    "$ log L(b|y,X) = \\sum_{i=1}^N \\left[ y_i log \\sigma(b^T X_i) + (1 - y_i) log (1 - \\sigma(b^T X_i)) \\right] $\n",
    "\n",
    "* Score Function $ S_N(b) $\n",
    "\n",
    "To obtain the maximum likelihood estimator, we need the gradient (or score) of the log likelihood function with respect to $ b $. The derivative of the log likelihood involves the derivative of the logistic function $ \\sigma(z) $, which is:\n",
    "\n",
    "$ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $\n",
    "\n",
    "Thus, the derivative of the log likelihood with respect to $ b $ is:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial b} log L(b|y,X) = \\sum_{i=1}^N X_i (y_i - \\sigma(b^T X_i)) $\n",
    "\n",
    "This expression $ \\sum_{i=1}^N X_i (y_i - \\sigma(b^T X_i)) $ is the score function $ S_N(b) $.\n",
    "\n",
    "* Expectation $ E[S_N(\\beta)] = 0 $\n",
    "\n",
    "Under the true parameter $ \\beta $, the expected value of $ y_i $ given $ X_i $ is $ \\sigma(\\beta^T X_i) $, and thus:\n",
    "\n",
    "$ E[y_i - \\sigma(\\beta^T X_i) | X_i] = \\sigma(\\beta^T X_i) - \\sigma(\\beta^T X_i) = 0 $\n",
    "\n",
    "Given the independence of $ y_i $ from $ y_j $ for $ i \\neq j $ given $ X $:\n",
    "\n",
    "$ E[S_N(\\beta)] = E\\left[\\sum_{i=1}^N X_i (y_i - \\sigma(\\beta^T X_i))\\right] = \\sum_{i=1}^N E[X_i (y_i - \\sigma(\\beta^T X_i))] = 0 $\n",
    "\n",
    "* Using $ S_N(\\beta) $ for GMM Estimation\n",
    "\n",
    "The score function can serve as the moment conditions for a GMM estimator. The GMM objective would minimize the quadratic form:\n",
    "\n",
    "$ \\min_b \\left(S_N(b)^T W S_N(b)\\right) $\n",
    "\n",
    "where $ W $ is a weighting matrix (often the inverse of the covariance matrix of the scores).\n",
    "\n",
    "* Efficiency Comparison\n",
    "\n",
    "- **Type I MLE**: The MLE obtained by maximizing the log likelihood is asymptotically efficient under regular conditions due to the information equality (it achieves the Cramer-Rao lower bound).\n",
    "- **GMM Using $ S_N(b) $**: If the GMM uses the same information as the MLE (i.e., uses the score functions as moments and uses the efficient weighting matrix related to the information matrix), it will be asymptotically equivalent to the MLE in terms of efficiency.\n",
    "- **GMM Using Other Moments**: If other moments are used (such as those based on higher moments or non-score-based moments), the GMM might be less efficient unless those moments incorporate additional relevant information not captured by the score function.\n",
    "\n",
    "* Conclusion\n",
    "\n",
    "The MLE is generally more efficient or at least as efficient as a GMM estimator using the same moments, due to the properties of the likelihood function and the efficient use of information. If the GMM uses different moments that do not align with those from the likelihood approach, it could be less efficient unless it compensates by capturing more comprehensive information about the underlying data generating process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a2566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
