{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Eleanor Adachi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Admin\n",
    "\n",
    "We created a fork of the main GitHub repository. Our code can be found here: https://github.com/eleanor-adachi/ARE212_Materials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercises\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.\n",
    "\n",
    "**Moore-Penrose Inverse**\n",
    "\n",
    "---\n",
    "\n",
    "A matrix $A^+$ is a \"Moore-Penrose\" generalized inverse if:\n",
    "\n",
    "- $AA^+A = A$;\n",
    "- $A^+AA^+ = A^+$;\n",
    "- $A^+A$ is symmetric; and\n",
    "- $AA^+$ is symmetric.\n",
    "\n",
    "**Full Rank Factorization**\n",
    "\n",
    "---\n",
    "\n",
    "Let $A$ be an $n\\times m$ matrix of rank $r$. If $A = LR$, where $L$ is an $n\\times r$ full column rank matrix, and $R$ is a $r\\times m$ full row rank matrix, then $LR$ is a full rank factorization of $A$.\n",
    "\n",
    "**Fact**\n",
    "\n",
    "---\n",
    "\n",
    "Provided only that $r>0$, the Moore-Penrose inverse $A^+ = R^{\\top}(L^{\\top}AR^{\\top})^{-1}L^{\\top}$ exists and is unique.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.) If $A$ is a matrix of zeros, what is $A^+$?\n",
    "\n",
    "For a matrix $A$ consisting entirely of zeros, its Moore-Penrose inverse, $A^+$, is also a matrix consisting entirely of zeros. This conclusion follows directly from the properties of the Moore-Penrose inverse:\n",
    "\n",
    "- $AA^+A = A$; multiplying $A^+$, which is a zero matrix, by $A$ from both sides will result in a zero matrix, satisfying this property.\n",
    "- $A^+AA^+ = A^+$; similarly, since $A$ is a zero matrix, $A^+$ remains unchanged and thus must also be a zero matrix to satisfy this property.\n",
    "- $A^+A$ is symmetric; a zero matrix multiplied by another zero matrix is still a zero matrix, which is inherently symmetric.\n",
    "- $AA^+$ is symmetric; likewise, this multiplication results in a zero matrix, which is symmetric.\n",
    "\n",
    "Hence, when $A$ is a matrix of zeros, $A^+$ is also a matrix of zeros.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.) Show  that if $X$ has full column rank, then $X^+ = (X^TX)^{-1}X^T$ (this is sometimes called the \"left inverse\"), and $X^+ X = I$.\n",
    "\n",
    "Given a matrix $X$ with full column rank, it means that all columns of $X$ are linearly independent. This implies that the matrix $X^TX$ is invertible. The Moore-Penrose inverse of $X$, $X^+$, satisfies the property that $XX^+X = X$.\n",
    "\n",
    "For matrices with full column rank, the Moore-Penrose inverse can be specifically expressed as $X^+ = (X^TX)^{-1}X^T$. This expression is sometimes referred to as the \"left inverse\" because when it is multiplied by $X$ from the left, it results in the identity matrix, $I$.\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "1. **Start with the expression for $X^+$**: \n",
    "\n",
    "   We have $X^+ = (X^TX)^{-1}X^T$.\n",
    "\n",
    "2. **Show that multiplying by $X$ yields $I$**:\n",
    "\n",
    "   Calculate $X^+X = [(X^TX)^{-1}X^T]X = (X^TX)^{-1}(X^TX) = I$.\n",
    "   \n",
    "   Here, the product $(X^TX)$ is invertible because $X$ has full column rank, ensuring that $X^TX$ is a full rank square matrix and thus invertible. Multiplying this invertible matrix by its inverse yields the identity matrix, $I$.\n",
    "\n",
    "This demonstrates that when $X$ has full column rank, its Moore-Penrose inverse $X^+$, when multiplied by $X$, yields the identity matrix, confirming that $X^+X = I$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3.) Use the result of (2) to solve for $b$ in the (matrix) form of theregression $y = Xb + u$ if $X^Tu = 0$.\n",
    "\n",
    "Given the regression equation $y = Xb + u$ where $X$ has full column rank and it's given that $X^Tu = 0$, we aim to solve for the coefficient vector $b$. We leverage the property of the Moore-Penrose inverse that if $X$ has full column rank, then $X^+ = (X^TX)^{-1}X^T$ and $X^+X = I$.\n",
    "\n",
    "1. **Starting from the regression equation**: \n",
    "\n",
    "   $$y = Xb + u$$\n",
    "\n",
    "2. **Apply the Moore-Penrose inverse of $X$ to both sides**:\n",
    "\n",
    "   Since we know $X^+X = I$, multiplying both sides by $X^+$ yields:\n",
    "\n",
    "   $$X^+y = X^+Xb + X^+u$$\n",
    "\n",
    "3. **Given that $X^Tu = 0$**:\n",
    "\n",
    "   This simplifies to:\n",
    "\n",
    "   $$X^+y = X^+Xb + 0$$\n",
    "\n",
    "   Which further simplifies to:\n",
    "\n",
    "   $$X^+y = b$$\n",
    "\n",
    "   Because $X^+X = I$.\n",
    "\n",
    "4. **Thus, the solution for $b$ is**:\n",
    "\n",
    "   $$b = X^+y$$\n",
    "\n",
    "   Where $X^+ = (X^TX)^{-1}X^T$ is the Moore-Penrose inverse of $X$.\n",
    "\n",
    "This method shows how to isolate the coefficient vector $b$ in the presence of a noise vector $u$ that is orthogonal to the column space of $X$ ($X^Tu = 0$). This approach assumes that $X$ has full column rank, allowing us to use the left inverse property of the Moore-Penrose inverse.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SUR "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.) If $\\Omega$ isn’t diagonal then there’s a sense in which the different equations in the system are dependent, since observing a realization of, say, $y_1$ may change our prediction of $y_2$. (This is why the system is called “seemingly” unrelated.) Describe this dependence formally.\n",
    "\n",
    "\n",
    "In the context of Seemingly Unrelated Regressions (SUR), when the covariance matrix $\\Omega$ is not diagonal, it indicates that the error terms of the different equations are correlated. This correlation leads to a dependence among the equations in the system, despite their appearance as unrelated. \n",
    "\n",
    "The covariance matrix $\\Omega$ represents the covariance of the error terms across different equations. A non-diagonal $\\Omega$ means that for some $i \\neq j$, the covariance $\\mathrm{cov}(u_i, u_j) \\neq 0$, where $u_i$ and $u_j$ are error terms from different equations in the system. This non-zero covariance implies a statistical dependence between the error terms, and consequently, between the equations themselves.\n",
    "\n",
    "Formally, the dependence can be described as follows:\n",
    "\n",
    "- Observing a realization of $y_1$ (which is influenced by $u_1$) can inform us about $u_2$, and hence, about the potential realization of $y_2$, if $\\mathrm{cov}(u_1, u_2) \\neq 0$. This is because the realization of $u_1$ provides information that can be used to update the expected value of $u_2$, reflecting a departure from independence.\n",
    "\n",
    "- This interdependence signifies that the errors (and therefore the outcomes) of one equation are informative about the errors (and outcomes) of another equation within the system. Thus, shocks or variations in one part of the system can have implications for other parts, which would not be the case if the equations were truly unrelated and the covariance matrix $\\Omega$ were diagonal.\n",
    "\n",
    "The presence of this dependence suggests that estimating the equations jointly, taking into account the correlation among the error terms, can yield more efficient and unbiased parameter estimates than estimating each equation separately without considering such correlations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Adapt the code in weighted_regression.ipynb so that the datagenerating process for $u$ can accommodate a general covariance matrix such as $\\Omega$, and let $X = T$. Estimate $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta: [ 0.49707795  0.99910028 -0.49457164]\n",
      "Residual variance: 0.22033587144331565\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from numpy.linalg import lstsq, pinv\n",
    "\n",
    "# Define parameters\n",
    "k = 3  # Number of observables in T\n",
    "N = 1000  # Sample size\n",
    "\n",
    "# Parameters for generating T\n",
    "mu = [0] * k\n",
    "Sigma = [[1, 0.5, 0],\n",
    "         [0.5, 2, 0],\n",
    "         [0, 0, 3]]  # Covariance matrix for T\n",
    "\n",
    "# Generate sample T\n",
    "T = multivariate_normal(mu, Sigma).rvs(N)\n",
    "\n",
    "# Define a general covariance matrix Omega for u\n",
    "Omega = [[0.2, 0, 0],  \n",
    "         [0, 0.2, 0],\n",
    "         [0, 0, 0.2]]\n",
    "\n",
    "# Generate u using Omega, taking the first component to maintain scalar outcome\n",
    "u = multivariate_normal(mean=np.zeros(k), cov=Omega).rvs(N)[:, 0]\n",
    "\n",
    "# Set X = T\n",
    "X = T\n",
    "\n",
    "# Define beta \n",
    "beta = np.array([0.5, 1, -0.5])  \n",
    "\n",
    "# Generate y\n",
    "y = X @ beta + u\n",
    "\n",
    "# Estimate beta using least squares\n",
    "b_est = lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Print the estimated beta\n",
    "print(f\"Estimated beta: {b_est}\")\n",
    "\n",
    "# Calculating residuals for variance estimation might not directly apply if dimensions of beta and T don't match directly\n",
    "# If needed, calculate residuals and estimate variance of beta\n",
    "e = y - X @ b_est\n",
    "print(f\"Residual variance: {np.var(e)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) How are the estimates obtained from this SUR system different from what one would obtain if one estimated equation by equation using OLS?\n",
    "\n",
    "The SUR approach can yield different estimates compared to estimating each equation separately using OLS when the error terms across equations are correlated. This correlation among error terms is what SUR explicitly accounts for, which can lead to efficiency gains in the parameter estimates.\n",
    "\n",
    "SUR leverages the covariance structure among the error terms across different equations. When these error terms are correlated, SUR, by considering this correlation, can provide more efficient estimates. Efficiency here refers to the variance of the estimator; more efficient estimators have smaller variances and are, therefore, closer to the true parameter value on average.\n",
    "\n",
    "Estimating each equation separately with OLS assumes that the error terms across different equations are uncorrelated. Under this assumption, OLS does not account for any potential information that could be gained from the error term correlations across equations. Consequently, if there indeed exists correlation across error terms (violating OLS assumptions when equations are related), OLS estimates might not be as efficient as those obtained from SUR.\n",
    "\n",
    "The key difference arises in situations where the error terms across equations are correlated. In such cases:\n",
    "\n",
    "- SUR can produce parameter estimates with smaller standard errors compared to OLS because it utilizes the information contained in the covariance structure of the errors.\n",
    "- OLS treats each equation as if it is standalone, ignoring the potential gains from understanding how the error terms across equations relate.\n",
    "\n",
    "If the error terms across the equations are actually uncorrelated, the SUR estimates and OLS estimates will be similar in terms of efficiency. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. “Plug-in” Kernel Bias Estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
