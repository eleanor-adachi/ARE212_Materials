{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Eleanor Adachi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Admin\n",
    "\n",
    "We created a fork of the main GitHub repository. Our code can be found here: https://github.com/eleanor-adachi/ARE212_Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercises\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 5.\n",
    "\n",
    "**Moore-Penrose Inverse**\n",
    "\n",
    "---\n",
    "\n",
    "A matrix $A^+$ is a \"Moore-Penrose\" generalized inverse if:\n",
    "\n",
    "- $AA^+A = A$;\n",
    "- $A^+AA^+ = A^+$;\n",
    "- $A^+A$ is symmetric; and\n",
    "- $AA^+$ is symmetric.\n",
    "\n",
    "**Full Rank Factorization**\n",
    "\n",
    "---\n",
    "\n",
    "Let $A$ be an $n\\times m$ matrix of rank $r$. If $A = LR$, where $L$ is an $n\\times r$ full column rank matrix, and $R$ is a $r\\times m$ full row rank matrix, then $LR$ is a full rank factorization of $A$.\n",
    "\n",
    "**Fact**\n",
    "\n",
    "---\n",
    "\n",
    "Provided only that $r>0$, the Moore-Penrose inverse $A^+ = R^{\\top}(L^{\\top}AR^{\\top})^{-1}L^{\\top}$ exists and is unique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.) If $A$ is a matrix of zeros, what is $A^+$?\n",
    "\n",
    "For a matrix $A$ consisting entirely of zeros, its Moore-Penrose inverse, $A^+$, is also a matrix consisting entirely of zeros. This conclusion follows directly from the properties of the Moore-Penrose inverse:\n",
    "\n",
    "- $AA^+A = A$; multiplying $A^+$, which is a zero matrix, by $A$ from both sides will result in a zero matrix, satisfying this property.\n",
    "- $A^+AA^+ = A^+$; similarly, since $A$ is a zero matrix, $A^+$ remains unchanged and thus must also be a zero matrix to satisfy this property.\n",
    "- $A^+A$ is symmetric; a zero matrix multiplied by another zero matrix is still a zero matrix, which is inherently symmetric.\n",
    "- $AA^+$ is symmetric; likewise, this multiplication results in a zero matrix, which is symmetric.\n",
    "\n",
    "Hence, when $A$ is a matrix of zeros, $A^+$ is also a matrix of zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.) Show  that if $X$ has full column rank, then $X^+ = (X^TX)^{-1}X^T$ (this is sometimes called the \"left inverse\"), and $X^+ X = I$.\n",
    "\n",
    "Given a matrix $X$ with full column rank, it means that all columns of $X$ are linearly independent. This implies that the matrix $X^TX$ is invertible. The Moore-Penrose inverse of $X$, $X^+$, satisfies the property that $XX^+X = X$.\n",
    "\n",
    "For matrices with full column rank, the Moore-Penrose inverse can be specifically expressed as $X^+ = (X^TX)^{-1}X^T$. This expression is sometimes referred to as the \"left inverse\" because when it is multiplied by $X$ from the left, it results in the identity matrix, $I$.\n",
    "\n",
    "*Proof:*\n",
    "\n",
    "1. **Start with the expression for $X^+$**: \n",
    "\n",
    "   We have $X^+ = (X^TX)^{-1}X^T$.\n",
    "\n",
    "2. **Show that multiplying by $X$ yields $I$**:\n",
    "\n",
    "   Calculate $X^+X = [(X^TX)^{-1}X^T]X = (X^TX)^{-1}(X^TX) = I$.\n",
    "   \n",
    "   Here, the product $(X^TX)$ is invertible because $X$ has full column rank, ensuring that $X^TX$ is a full rank square matrix and thus invertible. Multiplying this invertible matrix by its inverse yields the identity matrix, $I$.\n",
    "\n",
    "This demonstrates that when $X$ has full column rank, its Moore-Penrose inverse $X^+$, when multiplied by $X$, yields the identity matrix, confirming that $X^+X = I$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3.) Use the result of (2) to solve for $b$ in the (matrix) form of theregression $y = Xb + u$ if $X^Tu = 0$.\n",
    "\n",
    "Given the regression equation $y = Xb + u$ where $X$ has full column rank and it's given that $X^Tu = 0$, we aim to solve for the coefficient vector $b$. We leverage the property of the Moore-Penrose inverse that if $X$ has full column rank, then $X^+ = (X^TX)^{-1}X^T$ and $X^+X = I$.\n",
    "\n",
    "**Starting from the regression equation**: \n",
    "\n",
    "   $$y = Xb + u$$\n",
    "\n",
    "**Apply the Moore-Penrose inverse of $X$ to both sides**:\n",
    "\n",
    "   Since we know $X^+X = I$, multiplying both sides by $X^+$ yields:\n",
    "\n",
    "   $$X^+y = X^+Xb + X^+u$$\n",
    "\n",
    "**Given that $X^Tu = 0$**:\n",
    "\n",
    "   This simplifies to:\n",
    "\n",
    "   $$X^+y = X^+Xb + 0$$\n",
    "\n",
    "   Which further simplifies to:\n",
    "\n",
    "   $$X^+y = b$$\n",
    "\n",
    "   Because $X^+X = I$.\n",
    "\n",
    "**Thus, the solution for $b$ is**:\n",
    "\n",
    "   $$b = X^+y$$\n",
    "\n",
    "   Where $X^+ = (X^TX)^{-1}X^T$ is the Moore-Penrose inverse of $X$.\n",
    "\n",
    "This method shows how to isolate the coefficient vector $b$ in the presence of a noise vector $u$ that is orthogonal to the column space of $X$ ($X^Tu = 0$). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simultaneous Equations\n",
    "\n",
    "When we defined the general weighted regression, we didn’t assume anything about the dimension of the different objects except that they were ’conformable.’ So: consider\n",
    "\n",
    "(2) $y = X\\beta + u$, with $ET'u = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does our assumption of conformability then imply about the dimensions of $X$, $\\beta$, $T$, and $u$?\n",
    "\n",
    "In the context of the general weighted regression equation $y = X\\beta + u$, with the condition $E[T'u] = 0$ and $y$ being a $N \\times k$ matrix, the assumption of conformability dictates the following about the dimensions of $X$, $\\beta$, $T$, and $u$:\n",
    "\n",
    "- **$X$**: This is the matrix of independent variables or predictors. For the matrix multiplication $X\\beta$ to conform to $y$, $X$ must have dimensions of $N \\times m$, where $m$ is the number of independent variables. This dimensionality ensures that $X$ has one row per observation and one column for each independent variable.\n",
    "\n",
    "- **$\\beta$**: This matrix contains the regression coefficients associated with each independent variable across each of the $k$ dependent variables. For the product $X\\beta$ to result in a $N \\times k$ matrix (like $y$), $\\beta$ must be of dimension $m \\times k$, allowing each independent variable to potentially influence each dependent variable uniquely.\n",
    "\n",
    "- **$u$**: The error term must have the same dimension as $y$ to maintain conformability. Therefore, $u$ is a $N \\times k$ matrix.\n",
    "\n",
    "- **$T$**: Given the condition $E[T'u] = 0$, $T$ acts as a transformation or weighting matrix applied to the error terms. For $T'$ to multiply $u$, and considering the expectation, $T$ must be of dimension $p \\times N$, where $p$ could represent the number of constraints or transformations applied to the error terms. The condition implies a certain level of independence or orthogonality between the transformed errors and some other variables or components in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Could you use the estimator we developed in weighted_regression.ipynb to estimate this system of simultaneous equations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta coefficients:\n",
      " [0.49577531 1.00390216]\n",
      "\n",
      "Covariance matrix of b estimates:\n",
      " [[ 1.47560534e-05 -1.25255370e-05]\n",
      " [-1.25255370e-05  1.31066876e-05]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Number of observables in T and dependent variables in y\n",
    "k = 3 \n",
    "\n",
    "# Mean and covariance matrix for the multivariate normal distribution\n",
    "mu = [0] * k\n",
    "Sigma = [[1, 0.5, 0], [0.5, 2, 0], [0, 0, 3]]\n",
    "\n",
    "# Generate T \n",
    "T = multivariate_normal(mean=mu, cov=Sigma)\n",
    "\n",
    "# Error term u with its covariance\n",
    "u = multivariate_normal(mean=np.zeros(k), cov=0.2*np.eye(k))\n",
    "\n",
    "# Random matrix D for generating X\n",
    "D = np.random.random(size=(3, 2)) \n",
    "\n",
    "# Sample size\n",
    "N = 1000 \n",
    "\n",
    "# Generating samples for T and u\n",
    "T_sample = T.rvs(N)\n",
    "u_sample = multivariate_normal(mean=0, cov=0.2).rvs(N)\n",
    "\n",
    "# Generate X using a non-linear transformation of T\n",
    "X = (T_sample ** 3) @ D  \n",
    "\n",
    "# Define beta\n",
    "beta = np.array([1/2, 1])\n",
    "\n",
    "# Generate y with multiple dependent variables\n",
    "y = X @ beta + u_sample  \n",
    "\n",
    "# Estimate beta using least squares for each dependent variable\n",
    "b_estimates = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Print estimated beta coefficients\n",
    "print(\"Estimated beta coefficients:\\n\", b_estimates)\n",
    "\n",
    "# Calculate residuals\n",
    "e = y - X @ b_estimates\n",
    "\n",
    "# Calculate residuals variance\n",
    "var_e = np.var(e, ddof=X.shape[1])  # Adjusted for degrees of freedom\n",
    "\n",
    "# Calculating X'X inverse (X transpose X)\n",
    "XX_inv = np.linalg.inv(X.T @ X)\n",
    "\n",
    "# Calculating covariance matrix of b estimates\n",
    "vb = var_e * XX_inv\n",
    "\n",
    "print(\"\\nCovariance matrix of b estimates:\\n\", vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SUR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1.) If $\\Omega$ isn’t diagonal then there’s a sense in which the different equations in the system are dependent, since observing a realization of, say, $y_1$ may change our prediction of $y_2$. (This is why the system is called “seemingly” unrelated.) Describe this dependence formally.\n",
    "\n",
    "\n",
    "In the context of Seemingly Unrelated Regressions (SUR), when the covariance matrix $\\Omega$ is not diagonal, it indicates that the error terms of the different equations are correlated. This correlation leads to a dependence among the equations in the system, despite their appearance as unrelated. \n",
    "\n",
    "The covariance matrix $\\Omega$ represents the covariance of the error terms across different equations. A non-diagonal $\\Omega$ means that for some $i \\neq j$, the covariance $\\mathrm{cov}(u_i, u_j) \\neq 0$, where $u_i$ and $u_j$ are error terms from different equations in the system. This non-zero covariance implies a statistical dependence between the error terms, and consequently, between the equations themselves.\n",
    "\n",
    "Formally, the dependence can be described as follows:\n",
    "\n",
    "- Observing a realization of $y_1$ (which is influenced by $u_1$) can inform us about $u_2$, and hence, about the potential realization of $y_2$, if $\\mathrm{cov}(u_1, u_2) \\neq 0$. This is because the realization of $u_1$ provides information that can be used to update the expected value of $u_2$, reflecting a departure from independence.\n",
    "\n",
    "- This interdependence signifies that the errors (and therefore the outcomes) of one equation are informative about the errors (and outcomes) of another equation within the system. Thus, shocks or variations in one part of the system can have implications for other parts, which would not be the case if the equations were truly unrelated and the covariance matrix $\\Omega$ were diagonal.\n",
    "\n",
    "The presence of this dependence suggests that estimating the equations jointly, taking into account the correlation among the error terms, can yield more efficient and unbiased parameter estimates than estimating each equation separately without considering such correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Adapt the code in weighted_regression.ipynb so that the datagenerating process for $u$ can accommodate a general covariance matrix such as $\\Omega$, and let $X = T$. Estimate $\\beta$."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 20,
>>>>>>> e8fa4c43c1911581723f644bb0faffb5d1feed46
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Estimated beta: [ 0.49176698  0.99038921 -0.50164795]\n",
      "Residual variance: 0.19620509079910148\n"
=======
      "Dimensions of X: (1000, 3)\n",
      "Dimensions of beta: (3,)\n",
      "Dimensions of u: (1000,)\n",
      "Estimated beta: [ 0.48202934  1.00579006 -0.48408004]\n",
      "Residual variance: 0.6010909253684503\n"
>>>>>>> e8fa4c43c1911581723f644bb0faffb5d1feed46
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from numpy.linalg import lstsq, pinv\n",
    "\n",
    "# Define parameters\n",
    "k = 3  # Number of observables in T\n",
    "N = 1000  # Sample size\n",
    "D = np.random.random(size=(3,3)) # Generate random 3x3 matrix\n",
    "\n",
    "# Parameters for generating T\n",
    "mu = [0] * k\n",
    "Sigma = [[1, 0.5, 0],\n",
    "         [0.5, 2, 0],\n",
    "         [0, 0, 3]]  # Covariance matrix for T\n",
    "\n",
    "# Generate sample T\n",
    "T = multivariate_normal(mu, Sigma).rvs(N)\n",
    "\n",
    "# Define a general covariance matrix Omega for u\n",
    "A = np.random.rand(3,3)\n",
    "# Construct a positive semidefinite matrix B by multiplying A by its transpose\n",
    "Omega = np.dot(A, A.T)\n",
    "\n",
    "# Generate u using Omega, taking the first component to maintain scalar outcome\n",
    "u = multivariate_normal(mean=np.zeros(k), cov=Omega).rvs(N)[:, 0]\n",
    "# Set X = T\n",
    "X = T\n",
    "\n",
    "# Define beta \n",
    "beta = np.array([0.5, 1, -0.5])  \n",
    "\n",
    "\n",
    "# Generate y\n",
    "y = X @ beta + u\n",
    "\n",
    "# Estimate beta using least squares\n",
    "b_est = lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Print the estimated beta\n",
    "print(f\"Estimated beta: {b_est}\")\n",
    "\n",
    "# Calculating residuals for variance estimation might not directly apply if dimensions of beta and T don't match directly\n",
    "# If needed, calculate residuals and estimate variance of beta\n",
    "e = y - X @ b_est\n",
    "print(f\"Residual variance: {np.var(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) How are the estimates obtained from this SUR system different from what one would obtain if one estimated equation by equation using OLS?\n",
    "\n",
    "The SUR approach can yield different estimates compared to estimating each equation separately using OLS when the error terms across equations are correlated. This correlation among error terms is what SUR explicitly accounts for, which can lead to efficiency gains in the parameter estimates.\n",
    "\n",
    "SUR leverages the covariance structure among the error terms across different equations. When these error terms are correlated, SUR, by considering this correlation, can provide more efficient estimates. Efficiency here refers to the variance of the estimator; more efficient estimators have smaller variances and are, therefore, closer to the true parameter value on average.\n",
    "\n",
    "Estimating each equation separately with OLS assumes that the error terms across different equations are uncorrelated. Under this assumption, OLS does not account for any potential information that could be gained from the error term correlations across equations. Consequently, if there indeed exists correlation across error terms (violating OLS assumptions when equations are related), OLS estimates might not be as efficient as those obtained from SUR.\n",
    "\n",
    "The key difference arises in situations where the error terms across equations are correlated. In such cases:\n",
    "\n",
    "- SUR can produce parameter estimates with smaller standard errors compared to OLS because it utilizes the information contained in the covariance structure of the errors.\n",
    "- OLS treats each equation as if it is standalone, ignoring the potential gains from understanding how the error terms across equations relate.\n",
    "\n",
    "If the error terms across the equations are actually uncorrelated, the SUR estimates and OLS estimates will be similar in terms of efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. “Plug-in” Kernel Bias Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our discussion of bias of the kernel density estimator in lecture we\n",
    "constructed an “Oracle” estimator, which can be implemented when we\n",
    "know the true density $f$ that we’re trying to estimate. Of course, the Oracle estimator is only feasible when we don’t need it. What about the idea of using the same expression for bias as in\n",
    "the Oracle case, but replacing $f$ with our estimate $\\hat{f}$? Would this tell us anything useful? If so, under what conditions? What pitfalls might one encounter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this approach to be meaningful, certain conditions would have to be met:\n",
    "\n",
    "- **Consistency**: The kernel density estimator $\\hat{f}$ must be consistent, converging to $f$ as the sample size increases.\n",
    "- **Smoothness**: The true density $f$ should be sufficiently smooth.\n",
    "\n",
    "Several challenges might arise with this approach:\n",
    "- **Circular Reasoning**: Estimating the bias of $\\hat{f}$ using $\\hat{f}$ itself might lead to circular reasoning and uncertainty.\n",
    "- **Bandwidth Sensitivity**: The bias estimation is highly sensitive to bandwidth choice, which can significantly impact the outcome.\n",
    "- **Variance Ignorance**: This method might overlook the trade-off between bias and variance.\n",
    "- **Sample Size Sensitivity**: The effectiveness of the approach may be highly dependent on the sample size, with small samples potentially leading to misleading bias estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
